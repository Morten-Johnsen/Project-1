actg$event[0:25]
beta.hat$par
beta.hat
beta_hat$par
sum( -log( (1-actg$event)/actg$time )- beta_hat[2]*actg$time )
sum( -log( (1-actg$event)/actg$time )- beta_hat$par[2]*actg$time )
-log( (1-actg$event)/actg$time )- beta_hat$par[2]*actg$time
actg$time==0
-log( (1-actg$event)/actg$time )- beta_hat$par[2]*actg$time
actg$event[14]
actg$time[14]
-log( (1-actg$event[14]/actg$time[14]) )
-log( (1-actg$event[14]/actg$time[14]) ) - beta_hat$par[2]*actg$time[14]
-log( (1-actg$event[13]/actg$time[13]) ) - beta_hat$par[2]*actg$time[13]
-log( (1-actg$event[13])/actg$time[13]) ) - beta_hat$par[2]*actg$time[13]
sum( -log( (1-actg$event[actg$event==0])/actg$time[actg$event==0] )- beta_hat$par[2]*actg$time[actg$event==0] )
beta_hat$par
-log( (1-sum(actg$event))/sum(actg$time) ) - beta_hat$par[2] * sum(actg$time)
-log( (1-sum(actg$event))/sum(actg$time) ) - beta_hat$par[2] * sum(actg$tx)
knitr::opts_chunk$set(warning = F, fig.height = 4, message = F, dpi = 500)
rm(list = ls())
library(lubridate)
library(latex2exp)
library(circular)
library(tidyverse)
library(reshape)
library(pheatmap)
library(openair)
library(stringr)
library(numDeriv)
library(gridExtra)
library(openair)
library(PowerNormal)
library(sn)
library(gnorm)
library(emg)
library(survival)
library(survminer)
if (Sys.getenv("LOGNAME") == "mortenjohnsen"){
setwd("/Users/mortenjohnsen/OneDrive - Danmarks Tekniske Universitet/DTU/9. Semester/02418 - Statistical Modelling/Project-1/")
} else {
setwd("~/Documents/02418 Statistical Modelling/Assignments/Assignment 1/Project-1")
}
source("testDistribution.R")
log.data <- read.table("Logistic.txt", header=TRUE, sep="",
as.is=TRUE)
str(log.data)
#all data from one population:
bin.par <- nlminb(start = 0.1, objective = testDistribution
, x = c(sum(log.data$AIDS_yes), sum(log.data$n))
, distribution = "binomial")
#separately for the groups
x.AZT <- log.data %>%
filter(AZT == "Yes") %>%
dplyr::select(AIDS_yes, n) %>%
as.numeric()
AZT.par <- nlminb(start = 0.1, objective = testDistribution
, x = c(x.AZT[1], x.AZT[2])
, distribution = "binomial")
x.no.AZT <- log.data %>%
filter(AZT == "No") %>%
dplyr::select(AIDS_yes, n) %>%
as.numeric()
no.AZT.par <- nlminb(start = 0.1, objective = testDistribution
, x = c(x.no.AZT[1], x.no.AZT[2])
, distribution = "binomial")
p.hat <- sum(log.data$AIDS_yes)/sum(log.data$n)#bin.par$par
#Calculate expected values for this group based on each group size:
e.A.AZT <- log.data$n[log.data$AZT == "Yes"]*p.hat
e.A.no_AZT <- log.data$n[log.data$AZT == "No"]*p.hat
e.nA.AZT <- log.data$n[log.data$AZT == "Yes"]*(1-p.hat)
e.nA.no_AZT <- log.data$n[log.data$AZT == "No"]*(1-p.hat)
e <- c(e.A.AZT, e.A.no_AZT, e.nA.AZT, e.nA.no_AZT)
#### Without Continuity Correction: ####
#chi_squared <- sum((c(log.data$AIDS_yes,log.data$n-log.data$AIDS_yes)-e)^2/e)
#(chi_squared)
#probability of observing this chi-squared test statistic given that the null-hypothesis is true
#rows <- dim(log.data)[1]
#columns <- dim(log.data)[2]-1 #-1 because of the AZT column
#pchisq(chi_squared,df=(rows-1)*(columns-1),lower.tail=FALSE)
#### WITH CONTINUITY CORRECTION (correct): ####
#https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity
chi_squared_yates <- sum((abs(c(log.data$AIDS_yes,log.data$n-log.data$AIDS_yes)-e)-0.5)^2/e)
#probability of observing this chi-squared test statistic given that the null-hypothesis is true
rows <- dim(log.data)[1]
columns <- dim(log.data)[2]-1 #-1 because of the AZT column
cat(paste("chi_squared test statistic: ", chi_squared_yates, "\nP-value: ", pchisq(chi_squared_yates,df=(rows-1)*(columns-1),lower.tail=FALSE),
"\nThus there is a significant difference between the two groups. "))
### Result: There's a difference between the two groups.
#### direct test of proportions using R: ####
#log.data.for.chi <- log.data; log.data.for.chi$f <- log.data.for.chi$n - log.data.for.chi$AIDS_yes
#prop.test(log.data$AIDS_yes, log.data$n)
#or
#chisq.test(as.matrix(log.data.for.chi[,c(2,4)]))
#Likelihood-metode: #KOMMENTAR:START se ca. 30 linjer nede for "KOMMENTAR:SLUT"
x1 <- log.data$AIDS_yes[1]
x2 <- log.data$AIDS_yes[2]
n1 <- log.data$n[1] #har rettet fra [2]
n2 <- log.data$n[2]
(theta.hat <- log(x1/(n1-x1)*(n2-x2)/x2))
eta.start <- log(x2/(n2-x2))
L_theta <- function(eta){
return(-log(exp(theta.hat*x1+eta*(x1+x2))/((1+exp(theta.hat+eta))^n1*(1+exp(eta))^n2)))
}
(eta.hat <- nlminb(eta.start, objective = L_theta)$par)
#Now we can construct CIs based on the Likelihood function
L <- function(theta){
return(-log(exp(theta*x1+eta.hat*(x1+x2))/((1+exp(theta+eta.hat))^n1*(1+exp(eta.hat))^n2)))
}
theta.hat <- nlminb(start = 1, objective = L)$par
var <- solve(hessian(func <- L, x = theta.hat))
sd <- as.numeric(sqrt(var))
CI <- p.hat + c(-1,1)*qnorm(c(0.975))*sd/sqrt(sum(log.data$n)) #Så skal det her vel ikke bruges?
#jeg tænkte på, om ikke det var dette CI som kunne bruges til at teste vha. likelihood, om theta.hat overlapper med 0:
H.L.theta <- hessian(func = L, x = theta.hat)
V.L.theta <- solve(H.L.theta)
CI.theta <- theta.hat + c(-1,1) * qnorm(0.975) * sqrt(V.L.theta)
#KOMMENTAR:SLUT. Dette CI.theta kan vel bruges til at sige noget om, at da det ikke indeholder 0, så må effekten være signifikant?
#Det virker rigtigt. Hvilket slide/hvilken side i bogen er brugt? Slide 22, uge 4. Her er pi_1 = x1/n1 og pi_2 = x2/n2.
#Using the Wald statistic:
pf.p <- function(p, y){
nll <- testDistribution(p, y, distribution = "binomial")
return(nll)
}
#For p0
hessian.p0 <- hessian(func = pf.p, x = no.AZT.par$par, y = c(x.no.AZT[1], x.no.AZT[2]))
sd.p0 <- sqrt(diag(solve(hessian.p0)))
CI.p0 <- no.AZT.par$par + c(-1,1)*qnorm(0.975)*sd.p0
#For p1
hessian.p1 <- hessian(func = pf.p, x = AZT.par$par, y = c(x.AZT[1], x.AZT[2]))
sd.p1 <- sqrt(diag(solve(hessian.p1)))
CI.p1 <- AZT.par$par + c(-1,1)*qnorm(0.975)*sd.p1
print(cat("MLE of p_1 and 95% CI for group with AZT treatment: ", round(AZT.par$par,3), " [",round(CI.p1,3)[1],"; ",round(CI.p1,3)[2],"]"
,"\nMLE of p_0 and 95% CI for group with no AZT treatment: ", round(no.AZT.par$par,3), " [",round(CI.p0,3)[1],"; ",round(CI.p0,3)[2],"]"))
#Profile likelihoods to ensure that the Wald CI is an appropriate approximation.
#Estimate parameters in the model and report a confidence interval for the parameter
#describing the difference, compare with the result above.
#p_0: Probability of aids in control group
#p_1: Probability of aids in treatment group
#calculate likelihood
nll.p_0 <- function(beta, x = log.data$AIDS_yes[2], n = log.data$n[2]){
p <- exp(beta)/(1+exp(beta))
nll <- -sum(dbinom(x, size = n, prob = p, log = T))
return(nll)
}
opt.p_0 <- nlminb(start = 1, objective = nll.p_0, x = log.data$AIDS_yes[2], n = log.data$n[2])
beta_0 <- opt.p_0$par
nll.p_1 <- function(beta_1, beta_0, x = log.data$AIDS_yes[1], n = log.data$n[1]){
p <- exp(beta_0+beta_1)/(1+exp(beta_0+beta_1))
nll <- -sum(dbinom(x, size = n, prob = p, log = T))
}
opt.p_1 <- nlminb(start = 1
, objective = nll.p_1
, beta_0 = beta_0
, x = log.data$AIDS_yes[1]
, n = log.data$n[1])
beta_1 <- opt.p_1$par
(p_0 <- exp(beta_0)/(1 + exp(beta_0)))
(p_1 <- exp(beta_0 + beta_1) / (1 + exp(beta_0 + beta_1)))
logistic <- data.frame("AZT" = c(rep(1,170), rep(0,168))
,"AIDS_yes" = c(rep(c(1,0),c(25,170-25)), rep(c(1,0), c(44, 168-44))))
fit.glm <- glm(AIDS_yes ~ AZT, data = logistic, family = binomial)
print(cat(paste0("with glm model: ", coef(fit.glm)
,"\nBy hand (according to slide 19 lect 4): "
,"\nbeta_0 = ", beta_0, ", beta_1 = ", beta_1)))
#summary(fit.glm)
#Profile likelihoods
prof.b0 <- function(beta0, x = log.data$AIDS_yes[2], n = log.data$n[2]){
p <- exp(beta0)/(1+exp(beta0))
return(-sum(dbinom(x, size = n, prob = p, log = T)))
}
prof.b1 <- function(beta1, beta0, x = log.data$AIDS_yes[1], n = log.data$n[1]){
p <- exp(beta0+beta1)/(1+exp(beta0+beta1))
return(-sum(dbinom(x, size = n, prob = p, log = T)))
}
#beta intervals for examination
beta.zero.sims <- seq(-1.5,-0.6,0.01)
beta.one.sims <- seq(-1.3,-0.2,0.01)
#calculate profile likelihoods
pL.b0 <- sapply(beta.zero.sims, FUN = prof.b0)
pL.b1 <- sapply(beta.one.sims, FUN = prof.b1, beta0 = beta_0)
#plots
par(mfrow=c(1,2))
plot(beta.zero.sims
, -(pL.b0+max(-pL.b0))
, "l"
, main = TeX("Profile log-likelihood: $\\beta_0$"))
abline(h = -qchisq(0.95, df = 1)/2, lty = "dashed")
plot(beta.one.sims
, -(pL.b1+max(-pL.b1))
, "l"
, main = TeX("Profile log-likelihood: $\\beta_1$"))
abline(h = -qchisq(0.95, df = 1)/2, lty = "dashed")
sd_0 <- as.numeric(sqrt(diag(solve(hessian(beta_0, func = nll.p_0)))))
sd_1 <- as.numeric(sqrt(diag(solve(hessian(beta_1, func = nll.p_1, beta_0 = beta_0)))))
#Wald 95% CIs and profile-likelihoods with approx 95% CI
W.CI.0 <- round(beta_0 + c(-1,1)*qnorm(0.975)*sd_0,4)
W.CI.1 <- round(beta_1 + c(-1,1)*qnorm(0.975)*sd_1,4) #KOMMENTAR: CI.theta fra den tidligere kommentar er lig dette interval. Dette er vel netop, fordi theta i den sammenhæng beskriver forskellen på, om der er treatment eller ej, hvilket er det præcis samme som beta_1.
#W.CI.1;CI.theta
#Måske, forstår stadig ikke helt theta.hat fra tidligere, så det er svært at sige. Men du har nok ret.
#Direkte numerisk approksimation:
CI.0 <- round(c(min(beta.zero.sims[-(pL.b0+max(-pL.b0)) > -qchisq(0.95, df = 1)/2])
,max(beta.zero.sims[-(pL.b0+max(-pL.b0)) > -qchisq(0.95, df = 1)/2])), 4)
CI.1 <- round(c(min(beta.one.sims[-(pL.b1+max(-pL.b1)) > -qchisq(0.95, df = 1)/2])
,max(beta.one.sims[-(pL.b1+max(-pL.b1)) > -qchisq(0.95, df = 1)/2])), 4)
cat(paste("Wald Confidence intervals:"
,paste("\nbeta_0 = ", round(beta_0, 4), " [95% CI: ", W.CI.0[1],", ", W.CI.0[2],"]")
,paste("\nbeta_1 = ", round(beta_1, 4), " [95% CI: ", W.CI.1[1],", ", W.CI.1[2],"]")
,"\n\nLikelihood-based Confidence intervals:"
,paste0("\nbeta_0 = ", round(beta_0, 4), " [95% CI: ", CI.0[1],", ", CI.0[2],"]")
,paste0("\nbeta_1 = ", round(beta_1, 4), " [95% CI: ", CI.1[1],", ", CI.1[2],"]")))
confint(fit.glm)
par(mfrow = c(1,2))
plot(beta.zero.sims
, -(pL.b0+max(-pL.b0))
, "l"
,main = TeX("Profile log-likelihood: $\\beta_0$")
,xlab = expression(beta[0])
,ylab = "lp - max(lp)")
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = c(W.CI.0), col = 6)
text(x = W.CI.0[1]+0.2, y = -3, "Wald CI", col = 6)
text(x = CI.0[1]+0.1, y = -2.5, "CI", col = 2)
abline(v = c(CI.0), lty = "dashed", col = 2)
plot(beta.one.sims
, -(pL.b1+max(-pL.b1))
, "l"
,main = TeX("Profile log-likelihood: $\\beta_1$")
,xlab = expression(beta[1])
,ylab = "lp - max(lp)")
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = c(W.CI.1), col = 6)
text(x = W.CI.1[1]+0.2, y = -3, "Wald CI", col = 6)
text(x = CI.1[1]+0.1, y = -2.5, "CI", col = 2)
abline(v = c(CI.1),lty = "dashed", col = 2)
#tx: Treatment indicator. 1 = New treatment, 0 = Control treatment
#event: Indicator for AIDS or death. 1 = AIDS diagnosis or death, 0 = Otherwise
#time: Time to AIDS diagnosis or death. Days
#så tiden for event = 0 må angive at personen har været med i studiet time[X] dage uden at være enten død eller fået AIDS.
actg320 <- read.table("actg320.txt", header=TRUE, sep="",
as.is=TRUE)
#select time, event and tx as they are the only relevant variables in this project
actg <- actg320 %>%
dplyr::select(time, event, tx)
actg %>%
group_by(tx) %>%
summarise("Got AIDS or DIED" = sum(event),
"Proportion" = sum(event)/n(),
"Participants Given the Treatment" = n())
#page 310 in the course textbook
#With treatment: (p. 311)
actg_tx <- filter(actg, tx == 1)
actg_no_tx = filter(actg, tx == 0)
#this is larger than 2 and thus statistically significant (skal der sammenlignes med t-dist eller chi_squared?)
exp.nll <- function(theta, time = actg$time, event = actg$event){ #this isn't numerical? KOMMENTAR
nll <- -(sum(event)*log(1/theta) - sum(time/theta))
return(nll)
}
exp.nll.NUM <- function(theta, time = actg$time, event = actg$event){ #this is 8D
ll <- sum( dexp(time[event==1], 1/theta, log = T) ) + sum( pexp(time[event==0], 1/theta, lower.tail = F, log.p = T) )
return(-ll)
}
est <- nlminb(start = 1, objective = exp.nll)
est.NUM <- nlminb(start = 1, objective = exp.nll.NUM)
est.tx <- nlminb(start = 1, objective = exp.nll, time = actg_tx$time, event = actg_tx$event)
est.tx.NUM <- nlminb(start = 1, objective = exp.nll.NUM, time = actg_tx$time, event = actg_tx$event)
est.no.tx <- nlminb(start = 1, objective = exp.nll, time =actg_no_tx$time, event = actg_no_tx$event)
est.no.tx.NUM <- nlminb(start = 1, objective = exp.nll.NUM, time = actg_no_tx$time, event = actg_no_tx$event)
sd.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.NUM$par )))))
sd.tx.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.tx.NUM$par, time = actg_tx$time, event = actg_tx$event) ))))
sd.no.tx.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.no.tx.NUM$par, time = actg_no_tx$time, event = actg_no_tx$event) ))))
(est.tx.NUM$par - est.no.tx.NUM$par)/sqrt(sd.tx.NUM^2 + sd.no.tx.NUM^2) #311, compared to standard norm (z). the effect is significant :)
theta.hat.both <- est.NUM$par#to avoid having to alter all names
theta.hat.tx <- est.tx.NUM$par
theta.hat.no.tx <- est.no.tx.NUM$par
#weibull
wei.nll <- function(omega, time = actg$time, event = actg$event){
sigma <- omega[2]
theta <- omega[1]
p <- sigma^(-1)*time^(1/sigma-1)*theta^(-1/sigma) * exp(-time/theta)^(1/sigma)
P <- exp(-(time/theta)^(1/sigma))
nll <- -sum(log(p^event*P^(1-event)))
}
est.wei <- nlminb(start = c(2,2), objective = wei.nll, time = actg_tx$time, event = actg_tx$event)
#cumulative hazard function:
kaplan.meier <- survfit(Surv(time, event) ~ 1, data = actg)
kaplan.meier$time
kaplan.meier$cumhaz
est.cumhaz <- pexp(kaplan.meier$time, rate = 1/est$par)
compare.table <- tibble("time" = kaplan.meier$time, "KM cumhaz" = kaplan.meier$cumhaz, "exp cumhaz" = est.cumhaz)
ggplot(compare.table,aes(x = `exp cumhaz`, y = `KM cumhaz`))+
geom_step()+
geom_abline()
kaplan.meier.treatment <- survfit(Surv(time, event) ~ tx, data = actg)
est.cumhaz.tx <- pexp(kaplan.meier.treatment$time[-(1:228)], rate = 1/theta.hat.tx)
compare.table.treatment <- tibble("time" = kaplan.meier.treatment$time[-(1:228)], "KM cumhaz tx" = kaplan.meier.treatment$cumhaz[-(1:228)], "exp cumhaz tx" = est.cumhaz.tx)
ggplot(compare.table.treatment,aes(x = `exp cumhaz tx`, y = `KM cumhaz tx`))+
geom_step()+
geom_abline()
#weibull test for tx = 1
kaplan.meier.treatment <- survfit(Surv(time, event) ~ tx, data = actg)
time <- 1:max(kaplan.meier.treatment$time[-(1:228)])
sigma <- est.wei$par[2]
theta <- est.wei$par[1]
p <- sigma^(-1)*time^(1/sigma-1)*theta^(-1/sigma) * exp(-time/theta)^(1/sigma)
P <- exp(-(time/theta)^(1/sigma))
wei.cumhaz.tx <- cumsum(p/P)[kaplan.meier.treatment$time[-(1:228)]]
compare.table.treatment <- tibble("time" = kaplan.meier.treatment$time[-(1:228)], "KM cumhaz tx" = kaplan.meier.treatment$cumhaz[-(1:228)], "weibull cumhaz tx" = wei.cumhaz.tx)
ggplot(compare.table.treatment,aes(x = `weibull cumhaz tx`, y = `KM cumhaz tx`))+
geom_step()+
geom_abline()+
theme_bw()+
ggtitle("Weibull Survival Regression Model [Treatment Group]")+
labs(y = "Kaplan Meier Cumulative Hazard", x = "Weibull Regression Model Cumulative Hazard")
#numerical analysis above, analytical way below
kaplan.meier <- survfit(Surv(time, event) ~ tx, data = actg)
ggsurvplot_add_all(kaplan.meier
, data = actg
, conf.int = T
, risk.table = "abs_pct"
, ylim = c(0.8,1)
, pval = T
, ncensor.plot = T
,ggtheme = theme_bw()
,legend.labs = c("All", "No Treatment", "Treatment"))
fit <- survreg(Surv(time, event) ~ tx, data = actg,
dist = "exponential")
summary(fit)
confint(fit)
#Overvej residual plot
#Ifølge ovenstående:
#beta0 = 7.62 95% CI [7.38; 7.87]
#beta1 = 0.699 85% CI [0.28; 1.12]
# => Significant difference.
#ifølge oven- og nedenstående er der statistisk signifikant forskel.
surv_diff <- survdiff(Surv(time, event) ~ tx, data = actg)
surv_diff
#I hånden (jvf. slides fra uge 7):
#model: T = exp(B0 + B1*tx)*epsilon, epsilon ~ exp(1)
#Der kan opstilles to forskellige modeller afhængigt af tx = 0 eller tx = 1.
#tx = 0: E[T] = exp(b0)*epsilon
#tx = 1: E[T] = exp(b0 + b1)*epsilon
#Likelihood
nll.exp <- function(beta, time = actg$time, event = actg$event, treatment = actg$tx){
beta0 <- beta[1]
#dont want to make two functions so let beta1 = 0 if no treatment is not considered/used:
if (max(treatment) == 0){
beta1 <- 0
} else {
beta1 <- beta[2]
}
h <- exp(- beta0 - beta1 * treatment)
H <- time/exp(beta0 + beta1*treatment)
nll <- -sum(event*log(h) - H)
return(nll)
}
l.exp <- function(beta_tr, beta_nuis, time = actg$time, event = actg$event, treatment = actg$tx){
h <- exp(- beta_nuis - beta_tr * treatment)
H <- time/exp(beta_nuis + beta_tr*treatment)
ll <- sum(event*log(h) - H)
return(ll)
}
beta_hat <- nlminb(start = c(1,1)
, objective = nll.exp
, time = actg$time
, event = actg$event
, treatment = actg$tx)
beta_hat$par
#Comparing likelihoods with the result from bullet-point 4
beta_hat$objective #Ved ikke lige om der skal sammenlignes med de to modeller eller den ene? ahh
#måske skal man undersøge om begge værdier er statistisk signifikante således at vi kan argumentere for
#at der er tale om at behandlingen virker og sammenligne dette resultat med bullet-point 4.
#Calculate LRT
#optimise model without beta1 (no treatment):
beta_no_treatment_effect <- nlminb(start = 1
, objective = nll.exp
, time = actg$time
, event = actg$event
, treatment = rep(0, length(actg$tx)))
beta_no_treatment_effect$par
#LRT:
chi_squared <- - 2 * (beta_hat$objective - beta_no_treatment_effect$objective)
(p_value <- 1 - pchisq(chi_squared, df = 1))
#Here, we see that the treatment effect is statistically significant.
#### Bullet point 6 - Wald CI for the treatment parameters beta0 and beta1 ####
#Calculate profile likelihoods to ensure that the quadratic approximation by using Fischers Information matrix
#is acceptable.
beta.zero.sims <- seq(7.3,7.9,0.01)
beta.one.sims <- seq(0.2,1.2,0.01)
pL.beta0 <- apply(X = data.frame(beta.zero.sims,beta_hat$par[2]), MARGIN = 1 , FUN = nll.exp, time = actg$time, event = actg$event, treatment = actg$tx)
pL.beta1 <- apply(X = data.frame(beta_hat$par[1],beta.one.sims), MARGIN = 1 , FUN = nll.exp, time = actg$time, event = actg$event, treatment = actg$tx)
par(mfrow=c(1,2))
plot(beta.zero.sims
, -(pL.beta0+max(-pL.beta0))
, "l"
,main = "Profile likelihood for Beta 0"
,xlab = expression(beta[0])
,ylab = "lp - max(lp)"
,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
plot(beta.one.sims
, -(pL.beta1+max(-pL.beta1))
, "l"
,main = "Profile likelihood for beta 1"
,xlab = expression(beta[1])
,ylab = "lp - max(lp)"
,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
#CI:
H_tr <- hessian(l.exp, beta_hat$par[2], beta_nuis = beta_hat$par[1])
V_tr <- as.numeric(-1/H_tr)
( Wald.CI_tr <- round(beta_hat$par[2] + c(-1,1)*qnorm(0.975) * sqrt(V_tr), digits=2) )
H_nuis <- hessian(l.exp, beta_hat$par[1], beta_tr = beta_hat$par[2])
V_nuis <- as.numeric(-1/H_nuis)
( Wald.CI_nuis <- round(beta_hat$par[1] + c(-1,1)*qnorm(0.975) * sqrt(V_nuis), digits=2) )
Wald.CI <- matrix(c(Wald.CI_nuis, Wald.CI_tr), 2,2, byrow = T)#KOMMENTAR: Jeg har regnet wald på en anden måde og får nu det samme som for likelihood based CI. Det lader til, at '-1/H' og 'diag(solve())' er forskellen på de to. Den første giver det samme som likelihood based, den anden giver det samme som confint. 20 linjer nede er de alle sammen sammenlignet.
#SVAR: Den gamle metode bør være korrekt. Forskellen kommer fra om man regner H^-1 som en matrix eller et tal.
#er rimelig sikker på at den bør behandles som matrix, da det anvender mest data.
sd <- c(sqrt(V_nuis), sqrt(V_tr))
sd.former <- as.numeric(sqrt(diag(solve(hessian(beta_hat$par, func = nll.exp)))))
sd;sd.former
Wald.CI_former <- beta_hat$par + matrix(c(-1,1), 2,2, byrow = T) * matrix(qnorm(0.975)*sd.former, 2,2, byrow = F)
#Direkte numerisk approksimation:
CI.0 <- c(min(beta.zero.sims[-(pL.beta0+max(-pL.beta0)) > -qchisq(0.95, df = 1)/2])
,max(beta.zero.sims[-(pL.beta0+max(-pL.beta0)) > -qchisq(0.95, df = 1)/2]))
CI.1 <- c(min(beta.one.sims[-(pL.beta1+max(-pL.beta1)) > -qchisq(0.95, df = 1)/2])
,max(beta.one.sims[-(pL.beta1+max(-pL.beta1)) > -qchisq(0.95, df = 1)/2]))
cat(paste("Wald Confidence intervals:"
,paste("\nbeta_0 = ", round(beta_hat$par[1], 4), " [95% CI: ", Wald.CI[1,1],", ", Wald.CI[1,2],"]")
,paste("\nbeta_1 = ", round(beta_hat$par[2], 4), " [95% CI: ", Wald.CI[2,1],", ", Wald.CI[2,2],"]")
,"\n\nLikelihood-based Confidence intervals:"
,paste0("\nbeta_0 = ", round(beta_hat$par[1], 4), " [95% CI: ", CI.0[1],", ", CI.0[2],"]")
,paste0("\nbeta_1 = ", round(beta_hat$par[2], 4), " [95% CI: ", CI.1[1],", ", CI.1[2],"]")))
confint(fit);round(Wald.CI_former, digits=2)#KOMMENTAR: se her sammenligningen omtalt i den tidligere kommentar
#reinit the theta hats:
theta.hat.both<-0;theta.hat.tx<-0;theta.hat.no.tx<-0
(theta.hat.both <- sum(actg$time)/sum(actg$event) ) #this ain't numerical, xD
est.NUM$par
( theta.hat.tx <- sum(actg_tx$time)/sum(actg_tx$event) )
est.tx.NUM$par
#without treatment:
(theta.hat.no.tx <- sum(actg_no_tx$time)/sum(actg_no_tx$event) )
est.no.tx.NUM$par
( sigma.hat.tx <- theta.hat.tx/sum(actg$event[actg$tx==1])^(1/2) )
sd.tx.NUM
(sigma.hat.no.tx <- theta.hat.no.tx/sum(actg$event[actg$tx==0])^(1/2) )
sd.no.tx.NUM
#Wald statistic for comparison
(theta.hat.tx - theta.hat.no.tx)/sqrt(sigma.hat.tx^2 + sigma.hat.no.tx^2)
(est.tx.NUM$par - est.no.tx.NUM$par)/sqrt(sd.tx.NUM^2 + sd.no.tx.NUM^2)
exp.L.fun <- function(theta, time, event){
L <- (1/theta)^sum(event) * exp(-sum(time)/theta)
return(L)
}
thetas.tx <- seq(theta.hat.tx-2*sd.tx.NUM, theta.hat.tx+2.5*sd.tx.NUM, by = 0.02)
Ls <- sapply(thetas.tx, FUN = exp.L.fun, time = actg_tx$time, event = actg_tx$event)
plot(thetas.tx, Ls/max(Ls), main = "Profile likelihood for theta for exp dist",
xlab=expression(theta), ylab="norm L", lwd=1.0)
abline(a=0.15, b=0, col="red")
( theta.hat.tx+c(-1,1) * qnorm(0.975) * sd.tx.NUM )
#can't estimate MLEs... wtf
summary(fit)
beta_hat$par
beta_0_theo <- log(sum(actg$time * (1 - actg$tx)) / sum(actg$event*(1 - actg$tx)))
beta_1_theo <- log(sum(actg$time * actg$tx) / sum(actg$event * actg$tx)) -  beta_0_theo
beta_0_theo;beta_1_theo
#
# -log( (1-sum(actg$event))/sum(actg$time) ) - beta_hat$par[2] * sum(actg$tx)#beta_hat_0...
#
# ( -log( (actg$time - actg$event) / (actg$time * actg$tx) ) - beta_hat$par[1] ) / actg$tx#beta_hat_1...
( sd_tr <- sqrt(V_tr) )
par(mfrow = c(1,2))
plot(beta.zero.sims
, -(pL.beta0+max(-pL.beta0))
, "l"
,main = "Profile likelihood for Beta 0"
,xlab = expression(beta[0])
,ylab = "lp - max(lp)"
,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = Wald.CI[1,], col = 6)
text(x = Wald.CI[1,1]+.15, y = -2.4, "Wald CI", col = 6)
text(x = CI.0[1]+.05, y = -2.5, "CI", col = 2)
abline(v = c(CI.0), lty = "dashed", col = 2)
plot(beta.one.sims
, -(pL.beta1+max(-pL.beta1))
, "l"
,main = "Profile likelihood for beta 1"
,xlab = expression(beta[1])
,ylab = "lp - max(lp)"
,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = Wald.CI[2,], col = 6)
text(x = Wald.CI[2,1]+0.25, y = -1.6, "Wald CI", col = 6)
text(x = CI.1[1]+0.09, y = -1.7, "CI", col = 2)
abline(v = c(CI.1),lty = "dashed", col = 2)
#we compare the analytical result to the ones by glm survfit and numerical optimization:
summary(fit)
beta_hat$par
beta_0_theo <- log(sum(actg$time * (1 - actg$tx)) / sum(actg$event*(1 - actg$tx)))
beta_1_theo <- log(sum(actg$time * actg$tx) / sum(actg$event * actg$tx)) -  beta_0_theo
beta_0_theo;beta_1_theo
#we compare the analytical result to the ones by glm survfit and numerical optimization:
summary(fit)
beta_hat$par
beta_0_theo <- log(sum(actg$time * (1 - actg$tx)) / sum(actg$event*(1 - actg$tx)))
beta_1_theo <- log(sum(actg$time * actg$tx) / sum(actg$event * actg$tx)) -  beta_0_theo
beta_0_theo;beta_1_theo
#
# -log( (1-sum(actg$event))/sum(actg$time) ) - beta_hat$par[2] * sum(actg$tx)#beta_hat_0...
#
# ( -log( (actg$time - actg$event) / (actg$time * actg$tx) ) - beta_hat$par[1] ) / actg$tx#beta_hat_1...
