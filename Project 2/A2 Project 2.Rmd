---
title: "A2 Project 2"
author: "Johnsen & Johnsen"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, include = F}
knitr::opts_chunk$set(warning = F, fig.height = 4, message = F, dpi = 500)
rm(list = ls())
library(lubridate)
library(latex2exp)
library(circular)
library(tidyverse)
library(reshape)
library(pheatmap)
library(openair)
library(stringr)
library(numDeriv)
library(gridExtra)
library(openair)
library(PowerNormal)
library(sn)
library(gnorm)
library(emg)
library(survival)
library(survminer)
```

# Projekt 2: Survival Data

## Analysis of the Binary Data

### Read the data Logistic.txt into R.

```{r}
# if (Sys.getenv("LOGNAME") == "mortenjohnsen"){
#   setwd("/Users/mortenjohnsen/OneDrive - Danmarks Tekniske Universitet/DTU/9. Semester/02418 - Statistical Modelling/Project-1/Project 2/")
# } else {
#   setwd("~/Documents/02418 Statistical Modelling/Assignments/Assignment 1/Project-1")
# }

log.data <- read.table("Logistic.txt", header=TRUE, sep="", 
                       as.is=TRUE)

source("testDistribution.R")
```

### Fit a logistic regression model for the binary outcome AIDS="yes" versus AIDS="no" with the explanatory variable treatment with AZT (Yes, NO). Present the odds ratio for the effect of AZT on AIDS with 95% confidence interval and interpret the result in words

The logistic regression model is given by the likelihood function: $$
L(\theta) = \prod_i \left( \dfrac{\theta_i}{1-\theta_i} \right)^{y_i}(1-\theta_i)
$$ Here $\theta$ is calculated as: $$
\theta_i = \dfrac{e^{\beta_0 + \beta_1 t_{AZT}}}{1 + e^{\beta_0 + \beta_1 t_{AZT}}}
$$

```{r}
nll.log <- function(theta, event, n, treatment){
  
  beta0 <- theta[1]
  beta1 <- theta[2]
  
  theta <- exp(beta0 + beta1 * treatment)/(1 + exp(beta0 + beta1 * treatment))
  
  nll <- -sum(log(theta)*event  + log(1-theta)*(n - event))
  return(nll)
}

# For AZT treatment:
log.reg <- nlminb(start = c(1,1), objective = nll.log, event = log.data$AIDS_yes, n = log.data$n, treatment = c(1,0))

beta0 <- log.reg$par[1]
beta1 <- log.reg$par[2]
```

```{r}
logistic <- data.frame("AZT" = c(rep(1,170), rep(0,168))
                       ,"AIDS_yes" = c(rep(c(1,0),c(25,170-25)), rep(c(1,0), c(44, 168-44))))

fit.glm <- glm(AIDS_yes ~ AZT, data = logistic, family = binomial)
print(cat(paste0("with glm model: ", coef(fit.glm)
  ,"\nBy hand (according to slide 19 lect 4): "
  ,"\nbeta_0 = ", log.reg$par[1], ", beta_1 = ", log.reg$par[2])))

```

Calculate 95% wald confidence interval for $\beta_1$ and subsequently for the odds ratio. Hereafter we justify the use of the Wald CI by examining whether the score function show linearity around the MLE (p. ).

```{r}
sd <- sqrt(diag(solve(hessian(func = nll.log, x = c(beta0, beta1), event = log.data$AIDS_yes, n = log.data$n, treatment = c(1,0)))))
sd.beta0 <- sd[1]
sd.beta1 <- sd[2]

beta0.wald.CI <- beta0 + c(-1,1)*dnorm(0.975)*sd.beta0
beta1.wald.CI <- beta1 + c(-1,1)*dnorm(0.975)*sd.beta1

#Test regularity
score.nll.log <- function(theta){
  
  event <- log.data$AIDS_yes
  n <- log.data$n
  treatment <- c(1,0)
  
  beta0 <- theta[1]
  beta1 <- theta[2]
  
  theta <- exp(beta0 + beta1 * treatment)/(1 + exp(beta0 + beta1 * treatment))
  
  score <- -sum(event/theta - (1 - event)/(1 - theta))
  return(score)
}


beta0.test.range <- seq(beta0.wald.CI[1], beta0.wald.CI[2], abs(0.01*beta0))
beta1.test.range <- seq(beta1.wald.CI[1], beta1.wald.CI[2], abs(0.01*beta1))

res0 <- apply(cbind(beta0.test.range, beta1), MARGIN = 1, FUN = score.nll.log)

res1 <- apply(cbind(beta0, beta1.test.range), MARGIN = 1, FUN = score.nll.log)

par(mfrow = c(1,2))
plot(beta0.test.range, res0, main = TeX("Score linearity for $\\beta_0$")
     ,xlab = TeX("$\\beta_0$"), ylab = "Score", pch = 16)
abline(lm(res0 ~ beta0.test.range), col = 2, lwd = 2)
grid()

plot(beta1.test.range, res1, main = TeX("Score linearity for $\\beta_1$")
     ,xlab = TeX("$\\beta_1$"), ylab = "Score", pch = 16)
abline(lm(res1 ~ beta1.test.range), col = 2, lwd = 2)
grid()

```

Odds ratio = $exp(\beta_1)$ = `r exp(beta1)`, 95% CI [`r exp(c(beta1.wald.CI[1], beta1.wald.CI[2]))`]. Thus for the individuals receiving the AZT treatment, the odds of developing AIDS or dying is reduced by a factor 0.486 95% CI [0.45, 0.52].

```{r, include = F}
exp(beta1)
exp(c(beta1.wald.CI[1], beta1.wald.CI[2]))
```

### Test the hypothesis of no effect of AZT on AIDS using:

-   The likelihood ratio test
-   The Wald test
-   The score test

#### Likelihood ratio test

Calculate the objective value based on no treatment effect:

```{r}
nll.log.no.effect <- function(theta, event, n, treatment){
  
  beta0 <- theta[1]
  
  theta <- exp(beta0)/(1 + exp(beta0))
  
  nll <- -sum(log(theta)*event  + log(1-theta)*(n - event))
  return(nll)
} 

log.reg.no.effect <- nlminb(start = 1, objective = nll.log.no.effect
                        , event = sum(log.data$AIDS_yes)
                        , n = sum(log.data$n), treatment = 0)
```

Run the LRT (assuming regularity of the likelihoods (*page 36*)):

```{r}
chi.squared <- - 2 * (log.reg$objective - log.reg.no.effect$objective)
p.value.LRT <- 1 - pchisq(chi.squared, df = 1)

cat("The log ratio chi-square test statistic = ",chi.squared,
    "\np-value = ", p.value.LRT,
    "\n\nThe log-ratio test show that the effect of the treatment is significant on a significance level of alpha = 0.05.")
```

#### Wald test statistic

Calculating the wald test statistic (*p. 156*, or *p. 42 with* $\theta_0 = 0$).

```{r}
waldTestStatistic <- beta1/sd.beta1
wald.p.value <- 2*pnorm(waldTestStatistic)

cat("The wald test statistic = ",waldTestStatistic,
    "\np-value = ", wald.p.value,
    "\n\nThe wald test show that the effect of the treatment is significant on a significance level of alpha = 0.05.")
```

#### The score test

##From blackboard on 15/11
$$
\begin{split}
S_1 &= Y_1-\frac{m_1\cdot e^{\beta_0+\beta_1}}{1+e^{\beta_0+\beta_1}}+Y_2-\frac{m_2\cdot e^{\beta_0}}{1+e^{\beta_0}}\\
S_2 &= Y_1-\frac{m_1\cdot e^{\beta_0+\beta_1}}{1+e^{\beta_0+\beta_1}}
\end{split}\\
S=[S_1,S_2]^T
$$
Where $Y_1$ are events in treatment group and $Y_2$ are events in control group. And then $m_1$ are persons in treatment group and $m_2$ persons in control group. HOWEVER, as this is the score test, it is important to note that $\beta_1$ will not be set equal to the $\beta_1=-0.72$ from earlier but instead $\beta_{1,H_0}=0$ under our null hypothesis.

```{r}
Y_1 <- as.numeric(log.data$AIDS_yes[log.data$AZT=='Yes'])
Y_2 <- as.numeric(log.data$AIDS_yes[log.data$AZT=='No'])
m_1 <- as.numeric(log.data$n[log.data$AZT=='Yes'])
m_2 <- as.numeric(log.data$n[log.data$AZT=='No'])
beta0_score <- log.reg.no.effect$par #estimated is beta0, log.reg$par[1] for the model with treatment parameter included
beta1_score = 0 #estimated is beta1, log.reg$par[2]
P_1 <- exp(beta0_score+beta1_score) / (1+exp(beta0_score+beta1_score))
P_2 <- exp(beta0_score) / (1+exp(beta0_score))

S_1 <- Y_1 - m_1*P_1 + Y_2 - m_2*P_2
S_2 <- Y_1 - m_1*P_1
S <- c(S_1, S_2)


V_S_vect <- c(m_1*P_1*(1-P_1)+m_2*P_2*(1-P_2), m_1*P_1*(1-P_1)+0, m_1*P_1*(1-P_1)+0, m_1*P_1*(1-P_1))
V_S <- matrix(c(V_S_vect), nrow = 2, ncol = 2)

chi.squared.Score <- t(S)%*%solve(V_S)%*%S
(1 - pchisq(chi.squared.Score, df = 1))
p.value.Score <- as.numeric(1 - pchisq(chi.squared.Score, df = 1))

rbind(p.value.LRT, wald.p.value, p.value.Score)
```



Calculating the score function for the logistic regression model. 
$$
S(\theta) = \dfrac{1}{\theta} \sum_i y_i + \dfrac{1}{1+\theta} \sum_i 1-y_i
$$

```{r}
#Page 259

p <- function(beta,x){
  x <- rbind(1,x)
  out <- exp(beta %*% x)/(1 + exp(beta %*% x))
  return(as.numeric(out))
}

y <- c(rep(c(1,0),c(log.data$AIDS_yes[1], log.data$n[1]-log.data$AIDS_yes[1]))
       ,rep(c(1,0),c(log.data$AIDS_yes[2], log.data$n[2]-log.data$AIDS_yes[2])))
treatment <- rep(c(1,0), c(log.data$n[1],log.data$n[2]))

theta <- p(c(log.reg.no.effect$par,0), treatment)

score <- function(theta,treatment,y){
  score <- sum((y - theta)*treatment)
  return(score)
}

information <- function(theta,treatment,y){
  info <- sum(theta * (1 - theta) * treatment^2)
  return(info)
}

2*pnorm(score(theta,treatment,y) / sqrt(information(theta,treatment,y)))
```

The score test show that the null hypothesis stating $H_0$: $\theta = \theta_0$ - where $\theta_0$ is the population average - can be rejected on a significance level of $\alpha = 0.05$, thus showing that there's a difference in the risk of developing AIDS between the two treatment groups. This conclusion aligns with the results from the Wald test and the Log-Ratio Test. If the score test had shown that the p-value was above the $\alpha$ level, then it would have shown no difference between the $\theta_0$ and the actual MLE $\theta$. The intuitive interpretation of the Score test is that: *if the restricted estimator ($\theta_0$/the population level probability of the given event) is near the true MLE, then the score value $S(\theta_0)$, should not differ from 0 by more than $\approx$ 2 times the sample standard deviation.

The advantage of the Score statistic is that we do not need to perform calculations of the MLE, $\theta$, before being able to identify whether there is a difference between the two populations.

The score statistic is basically examining whether $S(\theta_0)$ is within $\approx 2 \sigma$ of 0. If it is, then it is possible that $\theta_0 = \theta$. Higher score function values, thus indicate a difference between the data set groups. 


## Analysis of the survival time data

### Descriptive statistics

#### Read the data actg320.txt into R

Read in the data:

```{r}
#tx: Treatment indicator. 1 = New treatment, 0 = Control treatment
#event: Indicator for AIDS or death. 1 = AIDS diagnosis or death, 0 = Otherwise
#time: Time to AIDS diagnosis or death. Days
#så tiden for event = 0 må angive at personen har været med i studiet time[X] dage uden at være enten død eller fået AIDS.
actg320 <- read.table("actg320.txt", header=TRUE, sep="", 
                      as.is=TRUE)

#select time, event and tx as they are the only relevant variables in this project
actg <- actg320 %>%
  dplyr::select(time, event, tx, cd4)
```

#### How many patients got AIDS or died in the two treatment groups? And how long was the total follow-up time in the two groups?

```{r}
actg %>%
  group_by(tx) %>%
  summarise("Got AIDS or DIED" = sum(event),
            "Proportion" = sum(event)/n(),
            "Participants Given the Treatment" = n(),
            "Total follow up time" = sum(time))
```

#### Plot the survival functions in the two treatment groups, which group seems to be doing best?

The treatment group seems to be doing the best (consider making this plot by hand instead of using survfit).

```{r}
kaplan.meier <- survfit(Surv(time, event) ~ tx, data = actg)
ggsurvplot(kaplan.meier
                   , data = actg
                   , conf.int = T
                   , risk.table = "abs_pct"
                   , ylim = c(0.8,1)
                   ,ggtheme = theme_bw()
                   ,legend.labs = c("No Treatment", "Treatment"))
```

#### Plot the cumulative incidence functions for the two groups, which plot would you prefer?

```{r}

ggsurvplot(kaplan.meier
                   , data = actg
                   , conf.int = T
                   , risk.table = "abs_pct"
                   ,ggtheme = theme_bw()
                   ,fun = "cumhaz"
                   ,legend.labs = c("No Treatment", "Treatment"))
```

#### Compare the survival in the two treatment groups using a log-rank test.

See slides 31-33 from week 6 for calculations by hand.

Manual Log-rank test:

```{r}
source("../kaplan-meier-manual.R")
source("../Manuel Log-rant (TEST).R")

logRank(data = actg, group = "tx", event = "event", time = "time")
```

p-value of 0.001, thus there is a significant difference between the two treatment groups based on a significance level of 0.05.

### Parametric survival models

#### Fit parametric survival models containing treatment (tx) and CD4 count (cd4) as explanatory variables

-   Try using the exponential, Weibull and log-logistic models, which one gave the best fit (and why)?

```{r}
exp.model <- survreg(Surv(time, event) ~ tx + cd4, data = actg, dist = "exponential")
weibull.model <- survreg(Surv(time, event) ~ tx + cd4, data = actg, dist = "weibull")
loglogistic.model <- survreg(Surv(time, event) ~ tx + cd4, data = actg, dist = "loglogistic")
```

```{r}
#summary(exp.model)

nll.exp <- function(theta
                    , time = actg$time
                    , event = actg$event
                    , treatment = actg$tx
                    , cd4 = actg$cd4){
  beta0 <- theta[1]
  beta1 <- theta[2]
  beta2 <- theta[3]
  
  h <- exp(- beta0 - beta1 * treatment - beta2 * cd4)
  H <- time/exp(beta0 + beta1 * treatment + beta2 * cd4)
  nll <- -sum(event*log(h) - H)
  return(nll)
}

exp.model.manual <- nlminb(start = c(1,1,1), objective = nll.exp)
sd.exp.manual <- sqrt(diag(solve(hessian(func = nll.exp, x = exp.model.manual$par))))
exp.model.manual.AIC <- - 2 * exp.model.manual$objective - 2 * 3

```

##### Weibull model

Use: slide 10 for f(t), S(t), h(t) and H(t) relationships, slide 19 for general log-likelihood expression for censored data, slide 55 for S(t) and *z*, slide 60 for h(t).

```{r}
#summary(weibull.model)

nll.wei <- function(theta
                    , time = actg$time
                    , event = actg$event
                    , treatment = actg$tx
                    , cd4 = actg$cd4){
  sigma <- exp(theta[1])  #estimate scale parameter on log-scale
  beta0 <- theta[2]
  beta1 <- theta[3]
  beta2 <- theta[4]
  
  h <-1/sigma * time^(1/sigma - 1) * exp(-1/sigma * (beta0 + beta1 * treatment + beta2 * cd4))
  H <- time^(1/sigma) * exp(-1/sigma * (beta0 + beta1 * treatment + beta2 * cd4))
  nll <- -sum(event*log(h) - H)
  return(nll)
}

weibull.model.manual <- nlminb(start = c(1,1,1,1), objective = nll.wei)
sd.weibull.manual <- sqrt(diag(solve(hessian(func = nll.wei, x = weibull.model.manual$par))))
weibull.model.manual.AIC <- 2 * weibull.model.manual$objective + 2 * length(weibull.model.manual$par) #3 parms + scale
```

```{r}
#summary(loglogistic.model)

nll.loglog <- function(theta
                    , time = actg$time
                    , event = actg$event
                    , treatment = actg$tx
                    , cd4 = actg$cd4){
  sigma <- theta[1]
  beta0 <- theta[2]
  beta1 <- theta[3]
  beta2 <- theta[4]
  
  z <- (log(time) - (beta0 + beta1 * treatment + beta2 * cd4))/sigma
  
  h <- exp(z)/(1 + exp(z)) * 1/sigma * 1/time
  S <- 1 / (1 + exp(z))
  H <- -log(S)
  #H <- time^(1/sigma) * exp(-1/sigma * (beta0 + beta1 * treatment + beta2 * cd4))
  nll <- -sum(event*log(h) - H)
  return(nll)
}

loglogistic.model.manual <- nlminb(start = c(1,1,1,1), objective = nll.loglog)
sd.loglogistic.manual <- sqrt(diag(solve(hessian(func = nll.loglog, x = loglogistic.model.manual$par))))
loglogistic.model.manual.AIC <- 2 * loglogistic.model.manual$objective + 2 * length(loglogistic.model.manual$par) #3 parms + scale
```

Examining the fits of the models through the cox-snell residuals

```{r}
par(mfrow = c(1,3))
actg$cox.snell.exp <- actg$time * exp(- exp.model$linear.predictors)
fit.exp <- survfit(Surv(cox.snell.exp, event == 1) ~ 1, data = actg)
plot(fit.exp$time, -log(fit.exp$surv), main = "Exponential Cox-Snell Check"
     ,xlab = TeX("$r_i$")
     ,ylab = TeX("-log($S_{KM}(r_i)$)"))
grid()
abline(a=0, b=1, col = 2, lwd = 2)

#slide 51 week 7 for weibull cox-snell
beta <- t(t(weibull.model.manual$par[2:4]))
x <- as.matrix(cbind(1, actg$tx, actg$cd4))
y.pred <- x %*% beta
y <- log(actg$time)

sigma <- exp(weibull.model.manual$par[1])

actg$r <- exp((y - y.pred)/sigma)
#slide 35 week 7 for the plot. [r_i, log(S_KM(r_i))], should yield a linear relationship with slope = 1.
fit.wei <- survfit(Surv(r, event == 1) ~ 1, data = actg)
#the output from the survfit function: time = r_i (as we input r_i as time). 
#the 'surv' output is the kaplan meier survival function of r_i:
plot(fit.wei$time, -log(fit.wei$surv), main = "Weibull Cox-Snell Check"
     ,xlab = TeX("$r_i$")
     ,ylab = TeX("-log($S_{KM}(r_i)$)"))
grid()
abline(a=0, b=1, col = 2, lwd = 2)

#slide 55 week 7 + slide 33 week 7 + slide 65 week 7
actg$cox.snell.loglog <- log( 1 + exp((log(actg$time) - loglogistic.model$linear.predictors) / loglogistic.model$scale))
fit.loglog <- survfit(Surv(cox.snell.loglog, event == 1) ~ 1, data = actg)
plot(fit.loglog$time, -log(fit.loglog$surv), main = "Log-logistic Cox-Snell Check"
     ,xlab = TeX("$r_i$")
     ,ylab = TeX("-log($S_{KM}(r_i)$)"))
grid()
abline(a=0, b=1, col = 2, lwd = 2)
```

Weibull and log-logistic seem quite similar. For both we still see a heavy tail, which is not accounted for in the model.

Use AIC to compare the models (exp excluded due to poor cox-snell fit):

```{r}
cat("Weibull Regression Model AIC: ", weibull.model.manual.AIC,
    "\nLog-logistic Regression Model AIC: ", loglogistic.model.manual.AIC)
```

*Here, the loglog model has the lower AIC, however we stick to the Weibull regression model as both these models have scale > 1, and thus show that the survival curve behaviour is monotonously decrease, and as such the loglog model has no modelwise advantage as compared to the weibull model. (Dobbelttjek dette)*

#### Using the survival model you chose, make a table of estimates and their 95% confidence intervals

```{r}
weibull.upper <- as.numeric(weibull.model.manual$par + qnorm(0.975)*sd.weibull.manual)
weibull.lower <- as.numeric(weibull.model.manual$par - qnorm(0.975)*sd.weibull.manual)

cat("beta_0 = ", weibull.model.manual$par[2], "95% CI [",weibull.lower[2],", ",weibull.upper[2],"]"
    ,"\nbeta_1 = ", weibull.model.manual$par[3], "95% CI [",weibull.lower[3],", ",weibull.upper[3],"]"
    ,"\nbeta_2 = ", weibull.model.manual$par[4], "95% CI [",weibull.lower[4],", ",weibull.upper[4],"]"
    ,"\nscale = ", exp(weibull.model.manual$par[1]), "95% CI [",exp(weibull.lower[1]),", ",exp(weibull.upper[1]),"]\n")
```

Table format:

```{r}
tibble("parameter" = c("beta_0", "beta_1", "beta_2", "sigma")
       , "theta hat" = c(weibull.model.manual$par[2:4], exp(weibull.model.manual$par[1]))
       , "lower CI (2.5%)" = c(weibull.lower[2:4], exp(weibull.lower[1]))
       , "upper CI (97.5%)" = c(weibull.upper[2:4], exp(weibull.upper[1])))
```

$\theta$ does not span 1, and thus the estimated weibull distribution is significantly different from simply using an exponential distribution.

Double-check whether the regularity assumption which is necessary for the use of Wald confidence intervals is appropriate, by examining the linearity of the score function at the MLE estimates:

```{r, include = F}
# score.nll.wei <- function(theta){
#   time = actg$time
#   event = actg$event
#   treatment = actg$tx
#   cd4 = actg$cd4
#   sigma <- exp(theta[1])  #estimate scale parameter on log-scale
#   beta0 <- theta[2]
#   beta1 <- theta[3]
#   beta2 <- theta[4]
#   
#   theta <- (beta0 + beta1 * treatment + beta2 * cd4)
#   
#   #score <- -sum(-event/sigma + time^(1/sigma)*exp(-theta/sigma)/sigma)
#   score <- -sum((-event + (time/theta)^(1/sigma))/(sigma*theta))
#   return(score)
# }
# 
# beta0 <- weibull.model.manual$par[2]
# beta1 <- weibull.model.manual$par[3]
# beta2 <- weibull.model.manual$par[4]
# sigma <- exp(weibull.model.manual$par[1])
# 
# beta0.wald.CI <- c(weibull.lower[2], weibull.upper[2])
# beta1.wald.CI <- c(weibull.lower[3], weibull.upper[3])
# beta2.wald.CI <- c(weibull.lower[4], weibull.upper[4])
# sigma.wald.CI <- exp(c(weibull.lower[1], weibull.upper[1]))
# 
# beta0.test.range <- seq(beta0.wald.CI[1], beta0.wald.CI[2], abs(0.01*beta0))
# beta1.test.range <- seq(beta1.wald.CI[1], beta1.wald.CI[2], abs(0.01*beta1))
# beta2.test.range <- seq(beta2.wald.CI[1], beta2.wald.CI[2], abs(0.01*beta2))
# 
# res0 <- apply(cbind(sigma, beta0.test.range, beta1, beta2), MARGIN = 1, FUN = score.nll.wei)
# 
# res1 <- apply(cbind(sigma, beta0, beta1.test.range, beta2), MARGIN = 1, FUN = score.nll.wei)
# 
# res2 <- apply(cbind(sigma, beta0, beta1, beta2.test.range), MARGIN = 1, FUN = score.nll.wei)
# 
# par(mfrow = c(1,3))
# plot(beta0.test.range, res0, main = TeX("Score linearity for $\\beta_0$")
#      ,xlab = TeX("$\\beta_0$"), ylab = "Score", pch = 16)
# abline(lm(res0 ~ beta0.test.range), col = 2, lwd = 2)
# grid()
# 
# plot(beta1.test.range, res1, main = TeX("Score linearity for $\\beta_1$")
#      ,xlab = TeX("$\\beta_1$"), ylab = "Score", pch = 16)
# abline(lm(res1 ~ beta1.test.range), col = 2, lwd = 2)
# grid()
# 
# plot(beta1.test.range, res1, main = TeX("Score linearity for $\\beta_2$")
#      ,xlab = TeX("$\\beta_2$"), ylab = "Score", pch = 16)
# abline(lm(res1 ~ beta1.test.range), col = 2, lwd = 2)
# grid()

```


#### Using your model compute the time ratio for the treatment effect. Similarly, compute the time ratio for the effect of increasing the CD4 count with 50. In both cases unceartainty evaluation (e.g. confidence intervals) should be included. Interpret the results in words

For the treatment effect:

```{r}
beta1 <- weibull.model.manual$par[3]

cat("Time Ratio for the treatment effect: TR(tx = 1, tx = 0) = ", exp(beta1), " 95% CI [",exp(c(weibull.lower[3])),",", exp(c(weibull.upper[3])),"]")
```

Based on this calculation of the time ratio of the treatment effect, it is evident that the treatment increase the median survival time by a factor 2.32 95% CI [1.32; 4.06].

For increasing the cd4 count by 50:

```{r}
beta2 <- weibull.model.manual$par[4]

cat("Time Ratio for the treatment effect: TR(tx = 1, tx = 0) = ", exp(beta2*50), " 95% CI [",exp(c(50*weibull.lower[4])),",", exp(c(50*weibull.upper[4])),"]")
```

Here we see that by increasing the cd4 count by 50, the median survival time is increased by a factor 2.8 95% CI [1.93,4.07].

#### Assess the goodness of fit of this model using a plot based on the Cox Snell residuals

This plot was already made earlier, but here it is again.

```{r}
par(mfrow = c(1,1))

plot(fit.wei$time, -log(fit.wei$surv), main = "Weibull Cox-Snell Check"
     ,xlab = TeX("$r_i$")
     ,ylab = TeX("-log($S_{KM}(r_i)$)"))
grid()
abline(a=0, b=1, col = 2, lwd = 2)
```

#### Give a graphical presentation of your model

```{r, fig.width = 8}
actg$cd4.cats <- cut(actg$cd4, breaks = c(0,100,max(actg$cd4)))
survival.functions <- survfit(Surv(time, event) ~ tx + cd4.cats, data = actg)

actg$cd4.category.num <- 0
actg$cd4.category.num[actg$cd4 < 100] <- 1
actg$cd4.category.num[actg$cd4 >= 100] <- 2

wei.survival <- function(theta, time, tx, cd4){
  beta0 <- theta[1]
  beta1 <- theta[2]
  beta2 <- theta[3]
  scale <- theta[4]
  
  out <- exp(-(time/exp(beta0 + beta1*tx + beta2*cd4))^(1/scale))
  return(out)
}

#for fixed values
step.size <- 0.5
theta <- c(weibull.model$coefficients,weibull.model$scale)
time.steps <- seq(0.5,max(actg$time), step.size) #lader den lige starte i 0.5 for at undgå bøvl med h.cum
first.sim <- wei.survival(theta, time.steps, 0, median(actg$cd4[actg$cd4 < 100 & actg$tx == 0]))
second.sim <- wei.survival(theta, time.steps, 0, median(actg$cd4[actg$cd4 > 100 & actg$tx == 0]))
third.sim <- wei.survival(theta, time.steps, 1, median(actg$cd4[actg$cd4 < 100 & actg$tx == 1]))
fourth.sim <- wei.survival(theta, time.steps, 1, median(actg$cd4[actg$cd4 > 100 & actg$tx == 1]))

#wei.survival(c(weibull.model$coefficients,weibull.model$scale), actg$time, actg$tx, actg$cd4)

par(mfrow = c(1,2))
plot(survival.functions, cumhaz = T, conf.int = T, col = 2:5, ylim = c(0,0.3), lwd = 1, main = "Weibull Regression Models and Kaplan Meier Cumulative Hazard")
grid()
f.weibull.t <- function(t, theta, x, scale){
  shape <- as.vector(theta %*% x)
  out <- shape/scale * (t/scale)^(shape-1) * exp(-(t/scale)^shape)
  return(out)
}

h.weibull.t <- function(t, theta, treatment, cd4, step.size){
  sigma <- exp(theta[1])
  beta0 <- theta[2]
  beta1 <- theta[3]
  beta2 <- theta[4]
  
  out <- cumsum(1/sigma * t^(1/sigma - 1) * exp(-1/sigma * (beta0 + beta1 * treatment + beta2 * cd4))*step.size)
  return(out)
}

lines(time.steps, h.weibull.t(time.steps, weibull.model.manual$par, 0, median(actg$cd4[actg$cd4 < 100 & actg$tx == 0]), step.size), col = 2 , lwd = 3)
lines(time.steps, h.weibull.t(time.steps, weibull.model.manual$par, 0, median(actg$cd4[actg$cd4 > 100 & actg$tx == 0]), step.size), col = 3, lwd = 3)
lines(time.steps, h.weibull.t(time.steps, weibull.model.manual$par, 1, median(actg$cd4[actg$cd4 < 100 & actg$tx == 1]), step.size), col = 4 , lwd = 3)
lines(time.steps, h.weibull.t(time.steps, weibull.model.manual$par, 1, median(actg$cd4[actg$cd4 > 100 & actg$tx == 1]), step.size), col = 5, lwd = 3)
legend(legend = c("cd4: 0-100, tx: 0", "cd4: 100+, tx: 0", "cd4: 0-100, tx: 1", "cd4: 100+, tx: 1"), lty = 1, "topleft", col = 2:5, lwd = 4)

plot(survival.functions, conf.int = T, col = 2:5, ylim = c(1,0.7), lwd = 1, main = "Weibull Regression Models and Kaplan Meier Survival Curves")
grid()
lines(time.steps, first.sim, col = 2 , lwd = 3)
lines(time.steps, second.sim, col = 3, lwd = 3)
lines(time.steps, third.sim, col = 4 , lwd = 3)
lines(time.steps, fourth.sim, col = 5, lwd = 3)
legend(legend = c("cd4: 0-100, tx: 0", "cd4: 100+, tx: 0", "cd4: 0-100, tx: 1", "cd4: 100+, tx: 1"), lty = 1, "bottomleft", col = 2:5, lwd = 4)

```
