---
title: "A1 Project 2"
author: "Johnsen & Johnsen"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r, include = F}
knitr::opts_chunk$set(warning = F, fig.height = 4, message = F, dpi = 500)
rm(list = ls())
library(lubridate)
library(latex2exp)
library(circular)
library(tidyverse)
library(reshape)
library(pheatmap)
library(openair)
library(stringr)
library(numDeriv)
library(gridExtra)
library(openair)
library(PowerNormal)
library(sn)
library(gnorm)
library(emg)
library(survival)
library(survminer)


if (Sys.getenv("LOGNAME") == "mortenjohnsen"){
  setwd("/Users/mortenjohnsen/OneDrive - Danmarks Tekniske Universitet/DTU/9. Semester/02418 - Statistical Modelling/Project-1/")
} else {
  setwd("~/Documents/02418 Statistical Modelling/Assignments/Assignment 1/Project-1")
}

source("testDistribution.R")
```

## Projekt 2: Survival Data

### Analysis of the Binary Data

#### Read the data Logistic.txt into R.


```{r}
log.data <- read.table("Logistic.txt", header=TRUE, sep="", 
                       as.is=TRUE)

str(log.data)
```

#### Fit the Binomial distribution to the data (i.e. consider all data as coming from the same population)

Fitting a binomial distribution to the full dataset and thus implicitly assuming no effect of the treatment:
```{r}
#all data from one population:
bin.par <- nlminb(start = 0.1, objective = testDistribution
                  , x = c(sum(log.data$AIDS_yes), sum(log.data$n))
                  , distribution = "binomial")
```


#### Fit the Binomial separately to the two distributions and test if there is a difference between the groups

Fitting the binomial separately to the two groups (treatment, no treatment):
```{r}
#separately for the groups
x.AZT <- log.data %>%
  filter(AZT == "Yes") %>%
  dplyr::select(AIDS_yes, n) %>%
  as.numeric()

AZT.par <- nlminb(start = 0.1, objective = testDistribution
                  , x = c(x.AZT[1], x.AZT[2])
                  , distribution = "binomial")

x.no.AZT <- log.data %>%
  filter(AZT == "No") %>%
  dplyr::select(AIDS_yes, n) %>%
  as.numeric()

no.AZT.par <- nlminb(start = 0.1, objective = testDistribution
                     , x = c(x.no.AZT[1], x.no.AZT[2])
                     , distribution = "binomial")

#OWN.IMPLMTTN created for ensuring that the equations in the presentation works exactly as expected. I've not saved
#the use of the equation.
own.implmttn <- function(theta, y = log.data$AIDS_yes, n = log.data$n){
  nll <- y*log(theta)+(n-y)*log(1-theta)
  return(-sum(nll))
}
```

Testing if there's a difference between the two groups:
```{r}
p.hat <- sum(log.data$AIDS_yes)/sum(log.data$n)#bin.par$par


#Calculate expected values for this group based on each group size:
e.A.AZT <- log.data$n[log.data$AZT == "Yes"]*p.hat
e.A.no_AZT <- log.data$n[log.data$AZT == "No"]*p.hat

e.nA.AZT <- log.data$n[log.data$AZT == "Yes"]*(1-p.hat)
e.nA.no_AZT <- log.data$n[log.data$AZT == "No"]*(1-p.hat)

e <- c(e.A.AZT, e.A.no_AZT, e.nA.AZT, e.nA.no_AZT)

#### Without Continuity Correction: ####
#chi_squared <- sum((c(log.data$AIDS_yes,log.data$n-log.data$AIDS_yes)-e)^2/e)
#(chi_squared)
#probability of observing this chi-squared test statistic given that the null-hypothesis is true
#rows <- dim(log.data)[1]
#columns <- dim(log.data)[2]-1 #-1 because of the AZT column
#pchisq(chi_squared,df=(rows-1)*(columns-1),lower.tail=FALSE)

#### WITH CONTINUITY CORRECTION (correct): ####
#https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity
chi_squared_yates <- sum((abs(c(log.data$AIDS_yes,log.data$n-log.data$AIDS_yes)-e)-0.5)^2/e)
#probability of observing this chi-squared test statistic given that the null-hypothesis is true
rows <- dim(log.data)[1]
columns <- dim(log.data)[2]-1 #-1 because of the AZT column
cat(paste("chi_squared test statistic: ", chi_squared_yates, "\nP-value: ", pchisq(chi_squared_yates,df=(rows-1)*(columns-1),lower.tail=FALSE),
          "\nThus there is a significant difference between the two groups. "))
### Result: There's a difference between the two groups.
```


```{r}
#Likelihood-metode: #KOMMENTAR:START se ca. 30 linjer nede for "KOMMENTAR:SLUT"
x1 <- log.data$AIDS_yes[1]
x2 <- log.data$AIDS_yes[2]
n1 <- log.data$n[1] #har rettet fra [2]
n2 <- log.data$n[2]

theta.hat <- log(x1/(n1-x1)*(n2-x2)/x2)

eta.start <- log(x2/(n2-x2))

L.both <- function(theta){
  theta.hat <- theta[1]
  eta.hat <- theta[2]
  nll <- -log(exp(theta.hat*x1+eta.hat*(x1+x2))/((1+exp(theta.hat+eta.hat))^n1*(1+exp(eta.hat))^n2))
  return(nll)
}
#KOMMENTAR:SLUT. Dette CI.theta kan vel bruges til at sige noget om, at da det ikke indeholder 0, så må effekten være signifikant?
#Det virker rigtigt. Hvilket slide/hvilken side i bogen er brugt? Slide 22, uge 3. Her er pi_1 = x1/n1 og pi_2 = x2/n2.

ests <- nlminb(start = c("theta" = 0, "eta" = 0), objective = L.both)
ests$par
H.likelihood.method <- hessian(func = L.both, x = ests$par)
V.likelihood.method <- diag(solve(H.likelihood.method))
sd.likelihood.method <- sqrt(V.likelihood.method)
(CI.theta <- theta.hat + c(-1,1) * qnorm(0.975) * sd.likelihood.method[1])
theta.hat

```

Result: By means of the usual non-likelihood method of proportion testing it has been shown that there is a statistically significant effect of the treatment.

Calculating MLE and uncertainty for $p_0$ and $p_1$. 

```{r}
#Using the Wald statistic:
pf.p <- function(p, y){
  nll <- testDistribution(p, y, distribution = "binomial")
  return(nll)
}
#For p0
hessian.p0 <- hessian(func = pf.p, x = no.AZT.par$par, y = c(x.no.AZT[1], x.no.AZT[2]))
sd.p0 <- sqrt(diag(solve(hessian.p0)))
CI.p0 <- no.AZT.par$par + c(-1,1)*qnorm(0.975)*sd.p0

#For p1
hessian.p1 <- hessian(func = pf.p, x = AZT.par$par, y = c(x.AZT[1], x.AZT[2]))
sd.p1 <- sqrt(diag(solve(hessian.p1)))
CI.p1 <- AZT.par$par + c(-1,1)*qnorm(0.975)*sd.p1

cat("MLE of p_1 and 95% CI for group with AZT treatment: ", round(AZT.par$par,3), " [",round(CI.p1,3)[1],"; ",round(CI.p1,3)[2],"]"
      ,"\nMLE of p_0 and 95% CI for group with no AZT treatment: ", round(no.AZT.par$par,3), " [",round(CI.p0,3)[1],"; ",round(CI.p0,3)[2],"]")
```

```{r, warnings = F, messages = F}
#Profile likelihoods to ensure that the Wald CI is an appropriate approximation.
```


#### Estimate parameters in the model (p0 probability of AIDS in control group, p1 probability of AIDS in treatment group) and report a confidence interval for the parameter describing the difference, compare with the result above.


Here $p_0$ indicate the risk of developing AIDS in the control group and $p_1$ indicate the risk of developing AIDS in the AZT treatment group.

$$
p_0 = \dfrac{e^{\beta_0}}{1+e^{\beta_0}}
$$
$$
p_1 = \dfrac{e^{\beta_0 + \beta_1}}{1+e^{\beta_0+\beta_1}}
$$

```{r}
#Estimate parameters in the model and report a confidence interval for the parameter 
#describing the difference, compare with the result above.
#p_0: Probability of aids in control group
#p_1: Probability of aids in treatment group

#COMMENT 9. dec: disse to kan kun bruges til at estimere MLE men formodentligt ikke usikkerheden på MLE.
#calculate likelihood
nll.p_0 <- function(beta, x = log.data$AIDS_yes[2], n = log.data$n[2]){
  p <- exp(beta)/(1+exp(beta))
  nll <- -sum(dbinom(x, size = n, prob = p, log = T))
  return(nll)
}
opt.p_0 <- nlminb(start = 1, objective = nll.p_0, x = log.data$AIDS_yes[2], n = log.data$n[2])
beta_0 <- opt.p_0$par

nll.p_1 <- function(beta_1, beta_0, x = log.data$AIDS_yes[1], n = log.data$n[1]){
  p <- exp(beta_0+beta_1)/(1+exp(beta_0+beta_1))
  nll <- -sum(dbinom(x, size = n, prob = p, log = T))
}
opt.p_1 <- nlminb(start = 1
                  , objective = nll.p_1
                  , beta_0 = beta_0
                  , x = log.data$AIDS_yes[1]
                  , n = log.data$n[1])
beta_1 <- opt.p_1$par

(p_0 <- exp(beta_0)/(1 + exp(beta_0)))
(p_1 <- exp(beta_0 + beta_1) / (1 + exp(beta_0 + beta_1)))
```


Comparing the results to the glm model i R:

```{r}
logistic <- data.frame("AZT" = c(rep(1,170), rep(0,168))
                       ,"AIDS_yes" = c(rep(c(1,0),c(25,170-25)), rep(c(1,0), c(44, 168-44))))

fit.glm <- glm(AIDS_yes ~ AZT, data = logistic, family = binomial)
print(cat(paste0("with glm model: ", coef(fit.glm)
  ,"\nBy hand (according to slide 19 lect 4): "
  ,"\nbeta_0 = ", beta_0, ", beta_1 = ", beta_1)))
#summary(fit.glm)
```


##### Calculating confidence intervals for the two beta parameters

Below can be seen the profile likelihood curves for the $\beta_0$ and $\beta_1$ estimates.

```{r}
###NEW NEW NEW NEW 9. dec
#USED FOR WALD CI IN NEXT BLOCK
nll.bin <- function(theta, event = log.data$AIDS_yes
                    , n = log.data$n
                    , treatment = c(1,0)){
  beta_0 <- theta[1]
  beta_1 <- theta[2]
  theta <- exp(beta_0+beta_1*treatment)/(1+exp(beta_0+beta_1*treatment))
  #nll <- -sum(dbinom(x, size = n, prob = p, log = T))
  nll <- -sum(log(theta)*event + (n-event)*log((1 - theta)))
  return(nll)
}
beta <- nlminb(start = c(0,0), objective = nll.bin)$par
sd <- sqrt(diag(solve(hessian(x = beta, func = nll.bin))))
###NEW NEW NEW NEW 9. dec
# NEW PL

nll.beta0.pfl <- function(beta_0, event = log.data$AIDS_yes
                    , n = log.data$n
                    , treatment = c(1,0)){
  fun.tmp <- function(beta_1, beta_0){
    theta <- exp(beta_0+beta_1*treatment)/(1+exp(beta_0+beta_1*treatment))
    nll.tmp <- -sum(log(theta)*event + (n-event)*log((1 - theta)))
    return(nll.tmp)
  }
  nll <- nlminb(start = c(1), objective = fun.tmp, beta_0 = beta_0)$objective
  return(nll)
}

nll.beta1.pfl <- function(beta_1, event = log.data$AIDS_yes
                    , n = log.data$n
                    , treatment = c(1,0)){
  fun.tmp <- function(beta_0, beta_1){
    theta <- exp(beta_0+beta_1*treatment)/(1+exp(beta_0+beta_1*treatment))
    nll.tmp <- -sum(log(theta)*event + (n-event)*log((1 - theta)))
    return(nll.tmp)
  }
  nll <- nlminb(start = c(1), objective = fun.tmp, beta_1 = beta_1)$objective
  return(nll)
}

#beta intervals for examination
beta.zero.sims <- seq(beta[1]-3*sd[1],beta[1]+3*sd[1],0.01)
beta.one.sims <- seq(beta[2]-3*sd[2],beta[2]+3*sd[2],0.01)
#calculate profile likelihoods
pL.b0 <- sapply(beta.zero.sims, FUN = nll.beta0.pfl)
pL.b1 <- sapply(beta.one.sims, FUN = nll.beta1.pfl)
#plots
par(mfrow=c(1,2))
plot(beta.zero.sims
     , -(pL.b0+max(-pL.b0))
     , "l"
     , main = TeX("Profile log-likelihood: $\\beta_0$"))
grid()
abline(h = -qchisq(0.95, df = 1)/2, lty = "dashed")
plot(beta.one.sims
     , -(pL.b1+max(-pL.b1))
     , "l"
     , main = TeX("Profile log-likelihood: $\\beta_1$"))
grid()
abline(h = -qchisq(0.95, df = 1)/2, lty = "dashed")
```

From these figures it can be concluded that the quadratic approximation of the CI through use of Fischers information matrix, is a sufficiently good approximation.

The Wald and likelihood-based confidence intervals for the two $\beta_i, i\in (0,1)$ parameters can be seen printed below.

```{r}
#sd_0 <- as.numeric(sqrt(diag(solve(hessian(beta_0, func = nll.p_0))))) #commented out 9. dec
#sd_1 <- as.numeric(sqrt(diag(solve(hessian(beta_1, func = nll.p_1, beta_0 = beta_0))))) #commented out 9. dec

#Wald 95% CIs and profile-likelihoods with approx 95% CI
W.CI.0 <- round(beta[1] + c(-1,1)*qnorm(0.975)*sd[1],4)
W.CI.1 <- round(beta[2] + c(-1,1)*qnorm(0.975)*sd[2],4) #KOMMENTAR: CI.theta fra den tidligere kommentar er lig dette interval. Dette er vel netop, fordi theta i den sammenhæng beskriver forskellen på, om der er treatment eller ej, hvilket er det præcis samme som beta_1.
#W.CI.1;CI.theta
#Måske, forstår stadig ikke helt theta.hat fra tidligere, så det er svært at sige. Men du har nok ret.

#Direkte numerisk approksimation:
CI.0 <- round(c(min(beta.zero.sims[-(pL.b0+max(-pL.b0)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.zero.sims[-(pL.b0+max(-pL.b0)) > -qchisq(0.95, df = 1)/2])), 4)
CI.1 <- round(c(min(beta.one.sims[-(pL.b1+max(-pL.b1)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.one.sims[-(pL.b1+max(-pL.b1)) > -qchisq(0.95, df = 1)/2])), 4)

cat(paste("Wald Confidence intervals:"
      ,paste("\nbeta_0 = ", round(beta_0, 4), " [95% CI: ", W.CI.0[1],", ", W.CI.0[2],"]")
      ,paste("\nbeta_1 = ", round(beta_1, 4), " [95% CI: ", W.CI.1[1],", ", W.CI.1[2],"]")
      ,"\n\nLikelihood-based Confidence intervals:"
      ,paste0("\nbeta_0 = ", round(beta_0, 4), " [95% CI: ", CI.0[1],", ", CI.0[2],"]")
      ,paste0("\nbeta_1 = ", round(beta_1, 4), " [95% CI: ", CI.1[1],", ", CI.1[2],"]")))
```

Comparing these to eachother, we see that the Wald CI is a very good approximation of the actual CIs. However, when comparing the estimates to our glm model we see that the $95\%$ CI for AZT is wider for the model estimate...

```{r}
confint(fit.glm)
```


Below can be seen the profile log-likelihoods for the two parameters, alongside their CIs.

```{r}
par(mfrow = c(1,2))
plot(beta.zero.sims
     , -(pL.b0+max(-pL.b0))
     , "l"
     ,main = TeX("Profile log-likelihood: $\\beta_0$")
     ,xlab = expression(beta[0])
     ,ylab = "lp - max(lp)")
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = c(W.CI.0), col = 6)
text(x = W.CI.0[1]+0.2, y = -3, "Wald CI", col = 6)
text(x = CI.0[1]+0.1, y = -2.5, "CI", col = 2)
abline(v = c(CI.0), lty = "dashed", col = 2)
plot(beta.one.sims
     , -(pL.b1+max(-pL.b1))
     , "l"
     ,main = TeX("Profile log-likelihood: $\\beta_1$")
     ,xlab = expression(beta[1])
     ,ylab = "lp - max(lp)")
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = c(W.CI.1), col = 6)
text(x = W.CI.1[1]+0.25, y = -3, "Wald CI", col = 6)
text(x = CI.1[1]+0.1, y = -2.5, "CI", col = 2)
abline(v = c(CI.1),lty = "dashed", col = 2)
```


### Analysis of the Survival Time Data

#### Read the data actg320.txt into R. If you are using RStudio you can use the "Import Dataset" button.

```{r}
#tx: Treatment indicator. 1 = New treatment, 0 = Control treatment
#event: Indicator for AIDS or death. 1 = AIDS diagnosis or death, 0 = Otherwise
#time: Time to AIDS diagnosis or death. Days
#så tiden for event = 0 må angive at personen har været med i studiet time[X] dage uden at være enten død eller fået AIDS.
actg320 <- read.table("actg320.txt", header=TRUE, sep="", 
                      as.is=TRUE)

#select time, event and tx as they are the only relevant variables in this project
actg <- actg320 %>%
  dplyr::select(time, event, tx)
```

#### How many patients got AIDS or died in the two treatment groups? What is the proportion of patients that got AIDS or died in the two group? Other relevant number that could be calculated?

```{r}
actg %>%
  group_by(tx) %>%
  summarise("Got AIDS or DIED" = sum(event),
            "Proportion" = sum(event)/n(),
            "Participants Given the Treatment" = n())
```

#### Fit an exponential distribution, using numerical methods, to the time of event (time) in the data set, remember to take into account that some of the data is censored (i.e. we only know that the time to the event is longer that the reported time). 1: Using all data (i.e. ignore the treatment effect) 2: Separately for the two treatments

Fitting an exponential model, where we account for censoring:
```{r}
#page 310 in the course textbook
```

Theta for each group individually:
```{r}
#With treatment: (p. 311)
actg_tx <- filter(actg, tx == 1)
actg_no_tx = filter(actg, tx == 0)
#this is larger than 2 and thus statistically significant (skal der sammenlignes med t-dist eller chi_squared?)
```
What we did the first time around was actaully using the analytical derivations and then using these to do the tests. I have commented the actual numerical analysis but have left the first results uncommented in order not to mess with the code. See exp.nll just below for "analytical optimization" and exp.ll.NUM for numerical optimization.

```{r}
exp.nll <- function(theta, time = actg$time, event = actg$event){ #this isn't numerical? KOMMENTAR
  nll <- -(sum(event)*log(1/theta) - sum(time/theta))
  return(nll)
}
exp.nll.NUM <- function(theta, time = actg$time, event = actg$event){ #this is 8D
  ll <- sum( dexp(time[event==1], 1/theta, log = T) ) + sum( pexp(time[event==0], 1/theta, lower.tail = F, log.p = T) )
  return(-ll) 
}

est <- nlminb(start = 1, objective = exp.nll)
est.NUM <- nlminb(start = 1, objective = exp.nll.NUM)
est.tx <- nlminb(start = 1, objective = exp.nll, time = actg_tx$time, event = actg_tx$event)
est.tx.NUM <- nlminb(start = 1, objective = exp.nll.NUM, time = actg_tx$time, event = actg_tx$event)
est.no.tx <- nlminb(start = 1, objective = exp.nll, time =actg_no_tx$time, event = actg_no_tx$event)
est.no.tx.NUM <- nlminb(start = 1, objective = exp.nll.NUM, time = actg_no_tx$time, event = actg_no_tx$event)

sd.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.NUM$par )))))
sd.tx.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.tx.NUM$par, time = actg_tx$time, event = actg_tx$event) ))))
sd.no.tx.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.no.tx.NUM$par, time = actg_no_tx$time, event = actg_no_tx$event) ))))

(est.tx.NUM$par - est.no.tx.NUM$par)/sqrt(sd.tx.NUM^2 + sd.no.tx.NUM^2) #311, compared to standard norm (z). the effect is significant :)

theta.hat.both <- est.NUM$par#to avoid having to alter all names
theta.hat.tx <- est.tx.NUM$par
theta.hat.no.tx <- est.no.tx.NUM$par

#LRT:
chi_squared <- - 2 * ((est.tx.NUM$objective + est.no.tx.NUM$objective) - est.NUM$objective)
(p_value <- 1 - pchisq(chi_squared, df = 1))
#Here, we see that the treatment effect is statistically significant.
```


```{r}
#weibull
wei.nll <- function(omega, time = actg$time, event = actg$event){
  sigma <- omega[2]
  theta <- omega[1]
  
  p <- sigma^(-1)*time^(1/sigma-1)*theta^(-1/sigma) * exp(-time/theta)^(1/sigma)
  P <- exp(-(time/theta)^(1/sigma))
  nll <- -sum(log(p^event*P^(1-event)))
}

est.wei <- nlminb(start = c(2,2), objective = wei.nll, time = actg_tx$time, event = actg_tx$event)

#cumulative hazard function:
kaplan.meier <- survfit(Surv(time, event) ~ 1, data = actg)
kaplan.meier$time
kaplan.meier$cumhaz
est.cumhaz <- pexp(kaplan.meier$time, rate = 1/est$par)
compare.table <- tibble("time" = kaplan.meier$time, "KM cumhaz" = kaplan.meier$cumhaz, "exp cumhaz" = est.cumhaz)

ggplot(compare.table,aes(x = `exp cumhaz`, y = `KM cumhaz`))+
  geom_step()+
  geom_abline()

kaplan.meier.treatment <- survfit(Surv(time, event) ~ tx, data = actg)
est.cumhaz.tx <- pexp(kaplan.meier.treatment$time[-(1:228)], rate = 1/theta.hat.tx)
compare.table.treatment <- tibble("time" = kaplan.meier.treatment$time[-(1:228)], "KM cumhaz tx" = kaplan.meier.treatment$cumhaz[-(1:228)], "exp cumhaz tx" = est.cumhaz.tx)

ggplot(compare.table.treatment,aes(x = `exp cumhaz tx`, y = `KM cumhaz tx`))+
  geom_step()+
  geom_abline()

#weibull test for tx = 1
kaplan.meier.treatment <- survfit(Surv(time, event) ~ tx, data = actg)
time <- 1:max(kaplan.meier.treatment$time[-(1:228)])
sigma <- est.wei$par[2]
theta <- est.wei$par[1]
p <- sigma^(-1)*time^(1/sigma-1)*theta^(-1/sigma) * exp(-time/theta)^(1/sigma)
P <- exp(-(time/theta)^(1/sigma))
wei.cumhaz.tx <- cumsum(p/P)[kaplan.meier.treatment$time[-(1:228)]]
compare.table.treatment <- tibble("time" = kaplan.meier.treatment$time[-(1:228)], "KM cumhaz tx" = kaplan.meier.treatment$cumhaz[-(1:228)], "weibull cumhaz tx" = wei.cumhaz.tx)

ggplot(compare.table.treatment,aes(x = `weibull cumhaz tx`, y = `KM cumhaz tx`))+
  geom_step()+
  geom_abline()+
  theme_bw()+
  ggtitle("Weibull Survival Regression Model [Treatment Group]")+
  labs(y = "Kaplan Meier Cumulative Hazard", x = "Weibull Regression Model Cumulative Hazard")
```

#### Compared the likelihood for the above models and conclude

```{r}
#numerical analysis above, analytical way below

```

#### Formulate a model where one parameter indicate the treatment effect, find the MLE and compare with the result above.

(e.g. E[T] = $e^\beta_0$ if control group and E[T] = $e^{\beta_0 + \beta_1}$ if treatment group)

```{r, fig.height=7}
kaplan.meier <- survfit(Surv(time, event) ~ tx, data = actg)
ggsurvplot_add_all(kaplan.meier
                   , data = actg
                   , conf.int = T
                   , risk.table = "abs_pct"
                   , ylim = c(0.8,1)
                   , pval = T
                   , ncensor.plot = T
                   ,ggtheme = theme_bw()
                   ,legend.labs = c("All", "No Treatment", "Treatment"))

fit <- survreg(Surv(time, event) ~ tx, data = actg,
               dist = "exponential")
summary(fit)
confint(fit)

#Overvej residual plot

#Ifølge ovenstående:
#beta0 = 7.62 95% CI [7.38; 7.87]
#beta1 = 0.699 85% CI [0.28; 1.12]
# => Significant difference.

#ifølge oven- og nedenstående er der statistisk signifikant forskel.
```

Are the effect of the treatment statistically significant?
```{r}
surv_diff <- survdiff(Surv(time, event) ~ tx, data = actg)
surv_diff
```
Yes it is significant.


Same calculations but now performed by hand:

```{r, fig.height=7}
#I hånden (jvf. slides fra uge 7):
#model: T = exp(B0 + B1*tx)*epsilon, epsilon ~ exp(1)
#Der kan opstilles to forskellige modeller afhængigt af tx = 0 eller tx = 1.
#tx = 0: E[T] = exp(b0)*epsilon
#tx = 1: E[T] = exp(b0 + b1)*epsilon

#Likelihood
nll.exp <- function(beta, time = actg$time, event = actg$event, treatment = actg$tx){
  beta0 <- beta[1]
  #dont want to make two functions so let beta1 = 0 if no treatment is not considered/used:
  if (max(treatment) == 0){
    beta1 <- 0
  } else {
    beta1 <- beta[2]
  }
  
  h <- exp(- beta0 - beta1 * treatment)
  H <- time/exp(beta0 + beta1*treatment)
  nll <- -sum(event*log(h) - H)
  return(nll)
}
l.exp <- function(beta_tr, beta_nuis, time = actg$time, event = actg$event, treatment = actg$tx){
  h <- exp(- beta_nuis - beta_tr * treatment)
  H <- time/exp(beta_nuis + beta_tr*treatment)
  ll <- sum(event*log(h) - H)
  return(ll)
}

beta_hat <- nlminb(start = c(1,1)
                   , objective = nll.exp
                   , time = actg$time
                   , event = actg$event
                   , treatment = actg$tx)
beta_hat$par
```


```{r, fig.height=7}
beta_hat$objective #Ved ikke lige om der skal sammenlignes med de to modeller eller den ene? ahh
#måske skal man undersøge om begge værdier er statistisk signifikante således at vi kan argumentere for
#at der er tale om at behandlingen virker og sammenligne dette resultat med bullet-point 4.

#Calculate LRT
#optimise model without beta1 (no treatment):
beta_no_treatment_effect <- nlminb(start = 1
                                   , objective = nll.exp
                                   , time = actg$time
                                   , event = actg$event
                                   , treatment = rep(0, length(actg$tx)))

beta_no_treatment_effect$par
#Comparing MLEs with previous result
exp(cumsum(beta_hat$par)); theta.hat.tx; theta.hat.no.tx; theta.hat.both; exp(beta_no_treatment_effect$par)
```


```{r, fig.height=7}
#LRT:
chi_squared <- - 2 * (beta_hat$objective - beta_no_treatment_effect$objective)
(p_value <- 1 - pchisq(chi_squared, df = 1))
#Here, we see that the treatment effect is statistically significant.

#### Bullet point 6 - Wald CI for the treatment parameters beta0 and beta1 ####
#Calculate profile likelihoods to ensure that the quadratic approximation by using Fischers Information matrix
#is acceptable.

#WALD CI
#SVAR: Den gamle metode bør være korrekt. Forskellen kommer fra om man regner H^-1 som en matrix eller et tal.
#er rimelig sikker på at den bør behandles som matrix, da det anvender mest data.
sd.former <- as.numeric(sqrt(diag(solve(hessian(beta_hat$par, func = nll.exp)))))

Wald.CI_former <- round(beta_hat$par + matrix(c(-1,1), 2,2, byrow = T) * matrix(qnorm(0.975)*sd.former, 2,2, byrow = F), 3)

#Her skal der laves pfl funktioner, som optimerer den parameter der ikke inputtes.
nll.exp.beta0 <- function(beta0, time = actg$time, event = actg$event, treatment = actg$tx){
  #dont want to make two functions so let beta1 = 0 if no treatment is not considered/used:
  fun.beta1.tmp <- function(beta1, beta0){
    h <- exp(- beta0 - beta1 * treatment)
    H <- time/exp(beta0 + beta1*treatment)
    nll.tmp <- -sum(event*log(h) - H)
    return(nll.tmp)
  }
  nll <- nlminb(start = 0, fun.beta1.tmp, beta0 = beta0)$objective

  return(nll)
}

nll.exp.beta1 <- function(beta1, time = actg$time, event = actg$event, treatment = actg$tx){
  #dont want to make two functions so let beta1 = 0 if no treatment is not considered/used:
  fun.beta0.tmp <- function(beta0, beta1){
    h <- exp(- beta0 - beta1 * treatment)
    H <- time/exp(beta0 + beta1*treatment)
    nll.tmp <- -sum(event*log(h) - H)
    return(nll.tmp)
  }
  nll <- nlminb(start = 0, fun.beta0.tmp, beta1 = beta1)$objective

  return(nll)
}

beta.zero.sims <- seq(beta_hat$par[1]-3*sd.former[1],beta_hat$par[1]+3*sd.former[1],0.0001)
beta.one.sims <- seq(beta_hat$par[2]-3*sd.former[2],beta_hat$par[2]+3*sd.former[2],0.0001)
#pL.beta0 <- apply(X = data.frame(beta.zero.sims,beta_hat$par[2]), MARGIN = 1 , FUN = nll.exp, time = actg$time, event = actg$event, treatment = actg$tx)
pL.beta0 <- sapply(X = beta.zero.sims, FUN = nll.exp.beta0)
#pL.beta1 <- apply(X = data.frame(beta_hat$par[1],beta.one.sims), MARGIN = 1 , FUN = nll.exp, time = actg$time, event = actg$event, treatment = actg$tx)
pL.beta1 <- sapply(X = beta.one.sims, FUN = nll.exp.beta1)

par(mfrow=c(1,2))
plot(beta.zero.sims
     , -(pL.beta0+max(-pL.beta0))
     , "l"
     ,main = "Profile likelihood for Beta 0"
     ,xlab = expression(beta[0])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
grid()
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
plot(beta.one.sims
     , -(pL.beta1+max(-pL.beta1))
     , "l"
     ,main = "Profile likelihood for beta 1"
     ,xlab = expression(beta[1])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
grid()
abline(h = -qchisq(0.95, df = 1)/2, col = 2)

```


#### Find the Wald confidence interval for the treatment parameter in the model above.

```{r}
#Direkte numerisk approksimation:
CI.0 <- round(c(min(beta.zero.sims[-(pL.beta0+max(-pL.beta0)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.zero.sims[-(pL.beta0+max(-pL.beta0)) > -qchisq(0.95, df = 1)/2])), 3)
CI.1 <- round(c(min(beta.one.sims[-(pL.beta1+max(-pL.beta1)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.one.sims[-(pL.beta1+max(-pL.beta1)) > -qchisq(0.95, df = 1)/2])),3)

cat(paste("Wald Confidence intervals:"
      ,paste("\nbeta_0 = ", round(beta_hat$par[1], 4), " [95% CI: ", Wald.CI_former[1,1],", ", Wald.CI_former[1,2],"]")
      ,paste("\nbeta_1 = ", round(beta_hat$par[2], 4), " [95% CI: ", Wald.CI_former[2,1],", ", Wald.CI_former[2,2],"]")
      ,"\n\nLikelihood-based Confidence intervals:"
      ,paste0("\nbeta_0 = ", round(beta_hat$par[1], 4), " [95% CI: ", CI.0[1],", ", CI.0[2],"]")
      ,paste0("\nbeta_1 = ", round(beta_hat$par[2], 4), " [95% CI: ", CI.1[1],", ", CI.1[2],"]")))
```

Comparing with the results from the survfit function:
```{r}
confint(fit);round(Wald.CI_former, digits=2)#KOMMENTAR: se her sammenligningen omtalt i den tidligere kommentar
```

Same as the wald CI. The likelihood based CI is a bit more narrow.

#### Derive the theoretical results for the models above, including the standard error estimates, use this to formulate and implement the profile likelihood function for the treatment parameter.
Theoretical results for the models of the exponential distributions. BULLET POINT 3-4.
Probabilities can be considered the probability of some data given a distribution, however, with likelihoods we want to estimate the distribution given some data. This is done by comparing the probability of observing the data given different $\theta s$. As a function of the unknown parameter the likelihood becomes:
$$
L(\theta)=P_{\theta}(X=x)
$$
<!-- p.21 was a great inspiration #KOMMENTAR, stort P eller lille p? jeg bruger ellers lille p for pdf og stort P for cdf -->

Where naturally x is the data, and $P_{\theta}$ is the pdf with parameter $\theta$. 
Usually when we work with likelihoods we simply consider all of the observations to be independent such that we can calculate the likelihood as a product of the probabilites of each observation in data = x given the same distribution $p_{\theta}$. So if X is a vector of *n* observations and $x_i$ represents one observation the likelihood is:
$$
L(\theta)=\Pi_{i=1}^n p_{\theta}(x_i)
$$
<!-- maybe p. 27 xD -->
This brings us to how we are to interpret the uncensored and censored values. The probability of any time value that is uncensored can simply be calculated as the probability of said time given event rate $\lambda = 1/\theta$ assuming an exponential distribution.
$$
L(\theta)_{uncensored}=p_{\theta,exp}(x_i)=\frac{1}{\theta} e^{-\frac{1}{\theta}x_i}
$$
For the uncensored values we will simply define the likelihood as the probability of observing a time value greater than the one registered for said person, thus using the cumulative distribution function:
$$
L(\theta)_{censored}=P_{\theta,exp}(x_i > t)=1 - P_{\theta,exp}(x_i\leq t)=1 - (1-e^{-\frac{1}{\theta}x_i}) =e^{-\frac{1}{\theta}x_i}
$$
We denote censoring by $\delta_i$ for observation $x_i$ where $\delta_i=0$ indicates censoring and $\delta_i=1$ indicates uncensored data. We can rewrite the two expressions above by combining their likelihoods respectively so that the former is the combined likelihood of all uncensored data and the latter is the combined likelihood of all censored data. The combined likelihood of these is then the likelihood of our unknown parameter $\theta$ given the data X:
$$
\begin{split}
L(\theta)&=\Pi_{i=1}^n p_{\theta,exp}(x_i)^{\delta_i} \cdot P_{\theta,exp}(x_i > t)^{1-\delta_i}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta} e^{-\frac{1}{\theta}x_i}\right)^{\delta_i} \cdot \left(e^{-\frac{1}{\theta}x_i}\right)^{1-\delta_i}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i\cdot\delta_i} \cdot e^{-\frac{1}{\theta}x_i\cdot(1-\delta_i)}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i\cdot\delta_i + (-\frac{1}{\theta}x_i\cdot(1-\delta_i))}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i\cdot\delta_i -\frac{1}{\theta}x_i+\frac{1}{\theta}x_i\cdot\delta_i}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i}\\
\end{split}
$$
As the base numbers in the two terms above are constant w.r.t. the summation, we can simply sum the exponents for each i:
$$
L(\theta)
=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i}=\left(\frac{1}{\theta}\right)^{\sum\delta_i} e^{-\frac{1}{\theta}\sum x_i}
$$
<!-- p. 310 to compare result -->
To determine the MLE and the fisher information we have to determine the score function which is known as the derivative of the log-likelihood. The log-likelihood is:
$$
\begin{split}
l(\theta)&=log(L(\theta))=log\left( \left(\frac{1}{\theta}\right)^{\sum\delta_i} e^{-\frac{1}{\theta}\sum x_i}\right)\\
&=log\left( \left( \frac{1}{\theta}\right)^{\sum\delta_i} \right) + log\left(e^{-\frac{1}{\theta}\sum x_i} \right)\\
&=\sum\delta_i\cdot log \left( \frac{1}{\theta}\right) - \frac{1}{\theta}\sum x_i
\end{split}
$$
In order to easily find the score function, do note that $log(\frac{1}{\theta})=log(\theta^{-1})=-log(\theta)$:

$$
\begin{split}
S(\theta)&
=\frac{\partial}{\partial \theta}log(L(\theta))
=\frac{\partial}{\partial \theta} \left( \sum\delta_i\cdot log \left( \frac{1}{\theta}\right) - \frac{1}{\theta}\sum x_i \right)\\
&=-\sum\delta_i\cdot\frac{1}{\theta}-\left(-\frac{1}{\theta^2}\sum x_i\right)\\
&=-\frac{\sum\delta_i}{\theta}+\frac{\sum x_i}{\theta^2}
\end{split}
$$

Setting this expression for the score function equal to zero we can find the MLE:
$$
\begin{split}
S(\theta)=0 &\Leftrightarrow -\frac{\sum\delta_i}{\theta}+\frac{\sum x_i}{\theta^2} = 0\\
&\Leftrightarrow\frac{\sum x_i}{\theta^2} = \frac{\sum\delta_i}{\theta}\\
&\Leftrightarrow\sum x_i=\theta\sum\delta_i\\
&\Leftrightarrow\hat{\theta}=\frac{\sum x_i}{\sum\delta_i}
\end{split}
$$
Thus we have determined the maximum likelihood estimate using an exponential model. This result can be applied to all three cases of the task: ignoring treament effect, for treatmeant effect, and for control group. NB! as the parameter was defined as the inverse of the event rate of the exponential function we have:
$\hat{\lambda} = \frac{1}{\hat{\theta}}=\frac{\sum\delta_i}{\sum x_i}$.

```{r}
#reinit the theta hats:
theta.hat.both<-0;theta.hat.tx<-0;theta.hat.no.tx<-0

(theta.hat.both <- sum(actg$time)/sum(actg$event) ) #this ain't numerical, xD
est.NUM$par

( theta.hat.tx <- sum(actg_tx$time)/sum(actg_tx$event) )
est.tx.NUM$par

#without treatment:
(theta.hat.no.tx <- sum(actg_no_tx$time)/sum(actg_no_tx$event) )
est.no.tx.NUM$par
```
All of the analytical results correspond to the numerical results obtained earlier.

Deriving an expression for the Fisher Information to obtain estimates for the standard deviation:
Fisher Information is defined as the negative derivative of the score function:
$$
\begin{split}
I(\theta)&=-\frac{\partial}{\partial\theta}S(\theta)=-\frac{\partial}{\partial\theta}\left(-\frac{\sum\delta_i}{\theta}+\frac{\sum x_i}{\theta^2}\right)\\
&=-\left( \frac{\sum\delta_i}{\theta^2}-\frac{2\sum x_i}{\theta^3}\right)\\
&=\frac{2\sum x_i}{\theta^3}-\frac{\sum\delta_i}{\theta^2}
\end{split}
$$
Now the observed Fisher Information is then taken at the MLE $\hat{\theta}=\frac{\sum x_i}{\sum\delta_i}$:
$$
\begin{split}
I(\hat{\theta})&=\frac{2\sum x_i}{\left(\frac{\sum x_i}{\sum\delta_i}\right)^3}-\frac{\sum\delta_i}{\left(\frac{\sum x_i}{\sum\delta_i}\right)^2}\\
&=\frac{2\sum x_i\sum\delta_i^3}{\sum x_i^3}-\frac{\sum\delta_i\sum\delta_i^2}{\sum x_i^2}\\
&=\frac{2\sum\delta_i^3}{\sum x_i^2}-\frac{\sum\delta_i^3}{\sum x_i^2}\\
&=\frac{\sum\delta_i^3}{\sum x_i^2}\\
&=\hat{\theta}^{-2}\sum\delta_i\\
&=\frac{\sum\delta_i}{\hat{\theta}^2}
\end{split}
$$
The variance of $\theta$ is given by $var(\theta)=I^{-1}(\theta)$ and as such since our parameter consists of only one value no matrix has to be solved and we can simply say:
$$
se(\hat{\theta})=\sqrt{I^{-1}(\hat{\theta})}=\sqrt{\frac{\hat{\theta}^2}{\sum\delta_i}}=\frac{\hat{\theta}}{\sqrt{\sum\delta_i}}
$$
The standard errors are determined analytically and compared to the numerically computed values:
```{r}

( sigma.hat.tx <- theta.hat.tx/sum(actg$event[actg$tx==1])^(1/2) )
sd.tx.NUM

(sigma.hat.no.tx <- theta.hat.no.tx/sum(actg$event[actg$tx==0])^(1/2) )
sd.no.tx.NUM
```

<!--using test on p. 311 -->
Testing if there is significance using the Wald statistic. Exactly the same result as when the numerical values were used.
```{r}
#Wald statistic for comparison
(theta.hat.tx - theta.hat.no.tx)/sqrt(sigma.hat.tx^2 + sigma.hat.no.tx^2)
(est.tx.NUM$par - est.no.tx.NUM$par)/sqrt(sd.tx.NUM^2 + sd.no.tx.NUM^2)
```
Profile likelihood for $\theta$:
```{r}
exp.L.fun <- function(theta, time, event){
  L <- (1/theta)^sum(event) * exp(-sum(time)/theta)
  return(L)
}
thetas.tx <- seq(theta.hat.tx-2*sd.tx.NUM, theta.hat.tx+2.5*sd.tx.NUM, by = 0.02)
Ls <- sapply(thetas.tx, FUN = exp.L.fun, time = actg_tx$time, event = actg_tx$event)
plot(thetas.tx, Ls/max(Ls), main = "Profile likelihood for theta for exp dist",
     xlab=expression(theta), ylab="norm L", lwd=1.0)
abline(a=0.15, b=0, col="red")

( theta.hat.tx+c(-1,1) * qnorm(0.975) * sd.tx.NUM )
```
#### ALL of the above derivations were for bullet points 3-4 (exponential distributions). Now we are moving on to bullet points 5-6 (lin. reg. exp).

A model that indicates treatment effect on survival time given by $E[T]=exp(\beta_0+\beta_1x)$ is called an exponential regression model as it can be linearized by taking the log: $log(T)=\beta_0+\beta_1x+\epsilon*$, where $\epsilon*$ is the error. Using slide 10 from week 7 the density function is given by the product of the hazard and the survival function: $f(t)=h(t)S(t)$. The hazard function in this case is given by the inverse of the time to event: $h(t)=exp(-\beta_0-\beta_1x)$ (slide 22). Further, the survival function is defined as the negative of the ¿cumulative? hazard function: $S(t)=exp(-H(t))$. The ¿cumulative? hazard function is simply the hazard function integrated w.r.t. time from 0 to t, however, as the hazard function does not depend on the time we simply have: $H(t)=t\cdot h(t)$. Note that this means we are apparently expecting the patients to have the same risk of dying at all times!
Knowing the ¿cumulative? hazard function it is possible to determine the survival function as $S(t)=exp(-H(t))=exp(-t\cdot h(t))=exp(-t\cdot exp(-\beta_0-\beta_1x))$, so:
$$
f(t)=h(t)S(t)=h(t)\cdot e^{-H(t)}=e^{-\beta_0-\beta_1x}\cdot e^{-t\cdot e^{(-\beta_0-\beta_1x)}}
$$
Then, if we denote an event as $d_i=1$ for uncensored events and $d_i=0$ for censored events then we can start deriving the likelihood function. Notice, that if all events were uncensored then the likelihood function should simply be the product of the f(t) for all of the given times as we are used to do it, however, the censored data complicates the process a bit. This is due to the fact that we need to find a way such that we can quantify the probability of surviving for longer that T: $P(T \leq t)$. Luckily, this is exactly what the survival function is for! Before writing up the likelihood we will summarize all of the definitions that will be used:
$$
\begin{split}
T&=e^{\beta_0+\beta_1x_i}=h^{-1}\\
S(t)&=e^{\frac{-t_i}{e^{\beta_0+\beta_1x_i}}}=e^{-H(t)}\\
h(t)&=e^{-\beta_0-\beta_1x_i}=T^{-1}\\
H(t)&=\frac{t_i}{e^{\beta_0+\beta_1x_i}}=-log(S(t))\\
f(t)&=h(t)S(t)
\end{split}
$$

We can write the likelihood:
$$
\begin{split}
L(\theta)&=\Pi_{i=1}^n f(t_i)^{\delta_i}S(t_i)^{1-\delta_i}\\
&=\Pi_{i=1}^n h(t_i)^{\delta_i}S(t_i)^{\delta_i}S(t_i)^{1-\delta_i}\\
&=\Pi_{i=1}^n h(t_i)^{\delta_i}S(t_i)
\end{split}
$$

Using the relationship between the ¿cumulative? hazard function and the survival function:
$$
L(\theta)=\Pi_{i=1}^n h^{\delta_i}e^{-H(t_i)}
$$
Notice that from this point onwards we will be using $h(t_i)=h$ as the hazard function is independent of the time $t$ anyways. The log-likelihood becomes:
$$
\begin{split}
l(\theta)=log(L(\theta))&=log\left(\Pi_{i=1}^n h^{\delta_i}e^{-H(t_i)}\right)\\
&=\sum_{i=1}^n log\left(h^{\delta_i}e^{-H(t_i)}\right)\\
&=\sum_{i=1}^n \delta_i\cdot log(h)+log\left(e^{-H(t_i)}\right)\\
&=\sum_{i=1}^n \delta_i\cdot log(h)-H(t_i)\\
&=\sum_{i=1}^n \delta_i\cdot log(e^{-\beta_0-\beta_1x_i})-t_i\cdot h\\
&=\sum_{i=1}^n \delta_i\cdot (-\beta_0-\beta_1x_i)-t_i\cdot e^{-\beta_0-\beta_1x_i}

\end{split}
$$
The two score functions will now be derived and set to zero. Firstly, w.r.t. $\beta_0$:
$$
\begin{split}
S(\theta)_{\beta_0}&=\frac{\partial}{\partial\beta_0}(l(\theta))\\
&=\sum_{i=1}^n -\delta_i+t_i\cdot e^{-\beta_0-\beta_1x_i}\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i&=\sum_{i=1}^n t_i\cdot e^{-\beta_0-\beta_1x_i}
\end{split}
$$

And w.r.t. $\beta_1$:
$$
\begin{split}
S(\theta)_{\beta_1}&=\frac{\partial}{\partial\beta_0}(l(\theta))\\
&=\sum_{i=1}^n -\delta_i\cdot x_i+t_i\cdot x_i\cdot e^{-\beta_0-\beta_1x_i}\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i\cdot x_i&=\sum_{i=1}^n t_i\cdot x_i\cdot e^{-\beta_0-\beta_1x_i}
\end{split}
$$
For $\beta_1$ only values of the treatment binary variable $x_i=1$ are of relevance as both summations equal 0 for $x_i=0$. Due to this we can make the exponential term independent of the summation by setting $x_i=1$ for the exponent. The reason as to why $x_i$ is not removed altogether is that the expression has to be true even when taken out of context:
$$
\begin{split}
\sum_{i=1}^n \delta_i\cdot x_i&=\sum_{i=1}^n t_i\cdot x_i\cdot e^{-\beta_0-\beta_1x_i} =\sum_{i=1}^n t_i\cdot x_i\cdot e^{-\beta_0-\beta_1}\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i\cdot x_i &=e^{-\beta_0-\beta_1} \sum_{i=1}^n t_i\cdot x_i\\
&\Leftrightarrow\\
e^{\beta_0+\beta_1}&=\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\\
&\Leftrightarrow\\
\beta_0+\beta_1&=log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)\\
&\Leftrightarrow\\
\beta_1&=log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)-\beta_0
\end{split}
$$
This can be inserted in the expression that was derived from the score equation where we differentiated w.r.t. $\beta_0$:
$$
\begin{split}
\sum_{i=1}^n \delta_i&=\sum_{i=1}^n t_i\cdot e^{-\beta_0-\beta_1x_i}\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i&=\sum_{i=1}^n t_i\cdot e^{-\beta_0-\left(log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)-\beta_0\right)x_i}\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i&=\sum_{i=1}^n t_i\cdot e^{-\beta_0+\beta_0\cdot x_i-log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)\cdot x_i}\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i&=\sum_{i=1}^n t_i\cdot e^{\beta_0\cdot (x_i-1)-log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)\cdot x_i}\\
\end{split}
$$
Notice that we have two distinct results for $x_i=0$ and $x_i=1$ respectively:
$$
\begin{split}
for& \;\;x_i=0:\;\; \beta_0\cdot (0-1)&-log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)\cdot 0=-\beta_0\\
for&\;\; x_i=1: \;\; \beta_0\cdot (1-1)&-log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)\cdot 1=-log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)=log\left(\frac{\sum_{i=1}^n \delta_i\cdot x_i}{\sum_{i=1}^n t_i\cdot x_i}\right)
\end{split}
$$
We can then divide the summation $\sum_{i=1}^n t_i\cdot e^{\beta_0\cdot (x_i-1)-log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)\cdot x_i}$ into two distinct sums:
$$
\begin{split}
for& \;\;x_i=0:\;\; \sum_{i=1}^n t_i\cdot (1-x_i)\cdot e^{-\beta_0}= e^{-\beta_0}\cdot\sum_{i=1}^n t_i\cdot (1-x_i)\\
for&\;\; x_i=1: \;\; \sum_{i=1}^n t_i\cdot x_i\cdot e^{log\left(\frac{\sum_{i=1}^n \delta_i\cdot x_i}{\sum_{i=1}^n t_i\cdot x_i}\right)}=e^{log\left(\frac{\sum_{i=1}^n \delta_i\cdot x_i}{\sum_{i=1}^n t_i\cdot x_i}\right)}\cdot\sum_{i=1}^n t_i\cdot x_i=\sum_{i=1}^n \delta_i\cdot x_i
\end{split}
$$
And so we get the following expression re-inserting this right hand side. NB! Now we have two separate sums on the right hand side:
$$
\begin{split}
\sum_{i=1}^n \delta_i&=\sum_{i=1}^n t_i\cdot e^{\beta_0\cdot (x_i-1)-log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)\cdot x_i}=e^{-\beta_0}\cdot\sum_{i=1}^n t_i\cdot (1-x_i)+\sum_{i=1}^n \delta_i\cdot x_i\\
\end{split}
$$
We can now isolate $\beta_0$:
$$
\begin{split}
\sum_{i=1}^n \delta_i&=e^{-\beta_0}\cdot\sum_{i=1}^n t_i\cdot (1-x_i)+\sum_{i=1}^n \delta_i\cdot x_i\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i-\sum_{i=1}^n \delta_i\cdot x_i&=e^{-\beta_0}\cdot\sum_{i=1}^n t_i\cdot (1-x_i)\\
&\Leftrightarrow\\
\sum_{i=1}^n \delta_i\cdot(1-x_i)&=e^{-\beta_0}\cdot\sum_{i=1}^n t_i\cdot (1-x_i)\\
&\Leftrightarrow\\
e^{\beta_0}&=\frac{\sum_{i=1}^n t_i\cdot (1-x_i)}{\sum_{i=1}^n \delta_i\cdot(1-x_i)}\\
&\Leftrightarrow\\
\beta_0&=log\left(\frac{\sum_{i=1}^n t_i\cdot (1-x_i)}{\sum_{i=1}^n \delta_i\cdot(1-x_i)}\right)
\end{split}
$$
Finally we have arrived at an analytical expression for the nuisance parameter $\beta_0$ and can use this to finally define an analytical expression for the treatment parameter:
$$
\begin{split}
\beta_1&=log\left(\frac{\sum_{i=1}^n t_i\cdot x_i}{\sum_{i=1}^n \delta_i\cdot x_i}\right)-\beta_0
\end{split}
$$
Where, naturally, $\beta_0=log\left(\frac{\sum_{i=1}^n t_i\cdot (1-x_i)}{\sum_{i=1}^n \delta_i\cdot(1-x_i)}\right)$ from above.

```{r}
#we compare the analytical result to the ones by glm survfit and numerical optimization:
summary(fit)
beta_hat$par
beta_0_theo <- log(sum(actg$time * (1 - actg$tx)) / sum(actg$event*(1 - actg$tx)))
beta_1_theo <- log(sum(actg$time * actg$tx) / sum(actg$event * actg$tx)) -  beta_0_theo
beta_0_theo;beta_1_theo
# 
# -log( (1-sum(actg$event))/sum(actg$time) ) - beta_hat$par[2] * sum(actg$tx)#beta_hat_0...
# 
# ( -log( (actg$time - actg$event) / (actg$time * actg$tx) ) - beta_hat$par[1] ) / actg$tx#beta_hat_1...
```


```{r}
par(mfrow = c(1,2))
plot(beta.zero.sims
     , -(pL.beta0+max(-pL.beta0))
     , "l"
     ,main = "Profile likelihood for Beta 0"
     ,xlab = expression(beta[0])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = Wald.CI_former[1,], col = 6)
text(x = Wald.CI_former[1,1]+.15, y = -2.3, "Wald CI", col = 6)
text(x = CI.0[1]+.05, y = -2.5, "CI", col = 2)
abline(v = c(CI.0), lty = "dashed", col = 2)
plot(beta.one.sims
     , -(pL.beta1+max(-pL.beta1))
     , "l"
     ,main = "Profile likelihood for beta 1"
     ,xlab = expression(beta[1])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = Wald.CI_former[2,], col = 6)
text(x = Wald.CI_former[2,1]+0.25, y = -1.5, "Wald CI", col = 6)
text(x = CI.1[1]+0.09, y = -1.7, "CI", col = 2)
abline(v = c(CI.1),lty = "dashed", col = 2)
```

**(Have not included our analysis based on the weibull distribution)**