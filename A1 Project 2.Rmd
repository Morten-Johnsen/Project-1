---
title: "A1 Project 2"
author: "Johnsen & Johnsen"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r, include = F}
knitr::opts_chunk$set(warning = F, fig.height = 4, message = F, dpi = 500)
rm(list = ls())
library(lubridate)
library(latex2exp)
library(circular)
library(tidyverse)
library(reshape)
library(pheatmap)
library(openair)
library(stringr)
library(numDeriv)
library(gridExtra)
library(openair)
library(PowerNormal)
library(sn)
library(gnorm)
library(emg)
library(survival)
library(survminer)


if (Sys.getenv("LOGNAME") == "mortenjohnsen"){
  setwd("/Users/mortenjohnsen/OneDrive - Danmarks Tekniske Universitet/DTU/9. Semester/02418 - Statistical Modelling/Project-1/")
} else {
  setwd("~/Documents/02418 Statistical Modelling/Assignments/Assignment 1/Project-1")
}

source("testDistribution.R")
```

## Projekt 2: Survival Data

### Analysis of the Binary Data

#### Read the data Logistic.txt into R.


```{r}
log.data <- read.table("Logistic.txt", header=TRUE, sep="", 
                       as.is=TRUE)

str(log.data)
```

#### Fit the Binomial distribution to the data (i.e. consider all data as coming from the same population)

Fitting a binomial distribution to the full dataset and thus implicitly assuming no effect of the treatment:
```{r}
#all data from one population:
bin.par <- nlminb(start = 0.1, objective = testDistribution
                  , x = c(sum(log.data$AIDS_yes), sum(log.data$n))
                  , distribution = "binomial")
```


#### Fit the Binomial separately to the two distributions and test if there is a difference between the groups

Fitting the binomial separately to the two groups (treatment, no treatment):
```{r}
#separately for the groups
x.AZT <- log.data %>%
  filter(AZT == "Yes") %>%
  dplyr::select(AIDS_yes, n) %>%
  as.numeric()

AZT.par <- nlminb(start = 0.1, objective = testDistribution
                  , x = c(x.AZT[1], x.AZT[2])
                  , distribution = "binomial")

x.no.AZT <- log.data %>%
  filter(AZT == "No") %>%
  dplyr::select(AIDS_yes, n) %>%
  as.numeric()

no.AZT.par <- nlminb(start = 0.1, objective = testDistribution
                     , x = c(x.no.AZT[1], x.no.AZT[2])
                     , distribution = "binomial")
```

Testing if there's a difference between the two groups:
```{r}
p.hat <- sum(log.data$AIDS_yes)/sum(log.data$n)#bin.par$par


#Calculate expected values for this group based on each group size:
e.A.AZT <- log.data$n[log.data$AZT == "Yes"]*p.hat
e.A.no_AZT <- log.data$n[log.data$AZT == "No"]*p.hat

e.nA.AZT <- log.data$n[log.data$AZT == "Yes"]*(1-p.hat)
e.nA.no_AZT <- log.data$n[log.data$AZT == "No"]*(1-p.hat)

e <- c(e.A.AZT, e.A.no_AZT, e.nA.AZT, e.nA.no_AZT)

#### Without Continuity Correction: ####
#chi_squared <- sum((c(log.data$AIDS_yes,log.data$n-log.data$AIDS_yes)-e)^2/e)
#(chi_squared)
#probability of observing this chi-squared test statistic given that the null-hypothesis is true
#rows <- dim(log.data)[1]
#columns <- dim(log.data)[2]-1 #-1 because of the AZT column
#pchisq(chi_squared,df=(rows-1)*(columns-1),lower.tail=FALSE)

#### WITH CONTINUITY CORRECTION (correct): ####
#https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity
chi_squared_yates <- sum((abs(c(log.data$AIDS_yes,log.data$n-log.data$AIDS_yes)-e)-0.5)^2/e)
#probability of observing this chi-squared test statistic given that the null-hypothesis is true
rows <- dim(log.data)[1]
columns <- dim(log.data)[2]-1 #-1 because of the AZT column
cat(paste("chi_squared test statistic: ", chi_squared_yates, "\nP-value: ", pchisq(chi_squared_yates,df=(rows-1)*(columns-1),lower.tail=FALSE),
          "\nThus there is a significant difference between the two groups. "))
### Result: There's a difference between the two groups.

#### direct test of proportions using R: ####
#log.data.for.chi <- log.data; log.data.for.chi$f <- log.data.for.chi$n - log.data.for.chi$AIDS_yes
#prop.test(log.data$AIDS_yes, log.data$n)
#or
#chisq.test(as.matrix(log.data.for.chi[,c(2,4)]))

#Likelihood-metode: #KOMMENTAR:START se ca. 30 linjer nede for "KOMMENTAR:SLUT"
x1 <- log.data$AIDS_yes[1]
x2 <- log.data$AIDS_yes[2]
n1 <- log.data$n[1] #har rettet fra [2]
n2 <- log.data$n[2]

(theta.hat <- log(x1/(n1-x1)*(n2-x2)/x2))

eta.start <- log(x2/(n2-x2))

L_theta <- function(eta){ 
  return(-log(exp(theta.hat*x1+eta*(x1+x2))/((1+exp(theta.hat+eta))^n1*(1+exp(eta))^n2)))
}
(eta.hat <- nlminb(eta.start, objective = L_theta)$par)

#Now we can construct CIs based on the Likelihood function
L <- function(theta){
  return(-log(exp(theta*x1+eta.hat*(x1+x2))/((1+exp(theta+eta.hat))^n1*(1+exp(eta.hat))^n2)))
}

theta.hat <- nlminb(start = 1, objective = L)$par

var <- solve(hessian(func <- L, x = theta.hat))
sd <- as.numeric(sqrt(var))
CI <- p.hat + c(-1,1)*qnorm(c(0.975))*sd/sqrt(sum(log.data$n)) #Så skal det her vel ikke bruges?

#jeg tænkte på, om ikke det var dette CI som kunne bruges til at teste vha. likelihood, om theta.hat overlapper med 0:
H.L.theta <- hessian(func = L, x = theta.hat)
V.L.theta <- solve(H.L.theta)
CI.theta <- theta.hat + c(-1,1) * qnorm(0.975) * sqrt(V.L.theta)
#KOMMENTAR:SLUT. Dette CI.theta kan vel bruges til at sige noget om, at da det ikke indeholder 0, så må effekten være signifikant?
#Det virker rigtigt. Hvilket slide/hvilken side i bogen er brugt? Slide 22, uge 4. Her er pi_1 = x1/n1 og pi_2 = x2/n2.
```

Result: By means of the usual non-likelihood method of proportion testing it has been shown that there is a statistically significant effect of the treatment.

Calculating MLE and uncertainty for $p_0$ and $p_1$. 

```{r}
#Using the Wald statistic:
pf.p <- function(p, y){
  nll <- testDistribution(p, y, distribution = "binomial")
  return(nll)
}
#For p0
hessian.p0 <- hessian(func = pf.p, x = no.AZT.par$par, y = c(x.no.AZT[1], x.no.AZT[2]))
sd.p0 <- sqrt(diag(solve(hessian.p0)))
CI.p0 <- no.AZT.par$par + c(-1,1)*qnorm(0.975)*sd.p0

#For p1
hessian.p1 <- hessian(func = pf.p, x = AZT.par$par, y = c(x.AZT[1], x.AZT[2]))
sd.p1 <- sqrt(diag(solve(hessian.p1)))
CI.p1 <- AZT.par$par + c(-1,1)*qnorm(0.975)*sd.p1

print(cat("MLE of p_1 and 95% CI for group with AZT treatment: ", round(AZT.par$par,3), " [",round(CI.p1,3)[1],"; ",round(CI.p1,3)[2],"]"
      ,"\nMLE of p_0 and 95% CI for group with no AZT treatment: ", round(no.AZT.par$par,3), " [",round(CI.p0,3)[1],"; ",round(CI.p0,3)[2],"]"))
```

```{r, warnings = F, messages = F}
#Profile likelihoods to ensure that the Wald CI is an appropriate approximation.
```


#### Estimate parameters in the model (p0 probability of AIDS in control group, p1 probability of AIDS in treatment group) and report a confidence interval for the parameter describing the difference, compare with the result above.


Here $p_0$ indicate the risk of developing AIDS in the control group and $p_1$ indicate the risk of developing AIDS in the AZT treatment group.

$$
p_0 = \dfrac{e^{\beta_0}}{1+e^{\beta_0}}
$$
$$
p_1 = \dfrac{e^{\beta_0 + \beta_1}}{1+e^{\beta_0+\beta_1}}
$$

```{r}
#Estimate parameters in the model and report a confidence interval for the parameter 
#describing the difference, compare with the result above.
#p_0: Probability of aids in control group
#p_1: Probability of aids in treatment group

#calculate likelihood
nll.p_0 <- function(beta, x = log.data$AIDS_yes[2], n = log.data$n[2]){
  p <- exp(beta)/(1+exp(beta))
  nll <- -sum(dbinom(x, size = n, prob = p, log = T))
  return(nll)
}
opt.p_0 <- nlminb(start = 1, objective = nll.p_0, x = log.data$AIDS_yes[2], n = log.data$n[2])
beta_0 <- opt.p_0$par

nll.p_1 <- function(beta_1, beta_0, x = log.data$AIDS_yes[1], n = log.data$n[1]){
  p <- exp(beta_0+beta_1)/(1+exp(beta_0+beta_1))
  nll <- -sum(dbinom(x, size = n, prob = p, log = T))
}
opt.p_1 <- nlminb(start = 1
                  , objective = nll.p_1
                  , beta_0 = beta_0
                  , x = log.data$AIDS_yes[1]
                  , n = log.data$n[1])
beta_1 <- opt.p_1$par

(p_0 <- exp(beta_0)/(1 + exp(beta_0)))
(p_1 <- exp(beta_0 + beta_1) / (1 + exp(beta_0 + beta_1)))
```


Comparing the results to the glm model i R:

```{r}
logistic <- data.frame("AZT" = c(rep(1,170), rep(0,168))
                       ,"AIDS_yes" = c(rep(c(1,0),c(25,170-25)), rep(c(1,0), c(44, 168-44))))

fit.glm <- glm(AIDS_yes ~ AZT, data = logistic, family = binomial)
print(cat(paste0("with glm model: ", coef(fit.glm)
  ,"\nBy hand (according to slide 19 lect 4): "
  ,"\nbeta_0 = ", beta_0, ", beta_1 = ", beta_1)))
#summary(fit.glm)
```


##### Calculating confidence intervals for the two beta parameters

Below can be seen the profile likelihood curves for the $\beta_0$ and $\beta_1$ estimates.

```{r}
#Profile likelihoods
prof.b0 <- function(beta0, x = log.data$AIDS_yes[2], n = log.data$n[2]){
  p <- exp(beta0)/(1+exp(beta0))
  return(-sum(dbinom(x, size = n, prob = p, log = T)))
}

prof.b1 <- function(beta1, beta0, x = log.data$AIDS_yes[1], n = log.data$n[1]){
  p <- exp(beta0+beta1)/(1+exp(beta0+beta1))
  return(-sum(dbinom(x, size = n, prob = p, log = T)))
}
#beta intervals for examination
beta.zero.sims <- seq(-1.5,-0.6,0.01)
beta.one.sims <- seq(-1.3,-0.2,0.01)
#calculate profile likelihoods
pL.b0 <- sapply(beta.zero.sims, FUN = prof.b0)
pL.b1 <- sapply(beta.one.sims, FUN = prof.b1, beta0 = beta_0)
#plots
par(mfrow=c(1,2))
plot(beta.zero.sims
     , -(pL.b0+max(-pL.b0))
     , "l"
     , main = TeX("Profile log-likelihood: $\\beta_0$"))
abline(h = -qchisq(0.95, df = 1)/2, lty = "dashed")
plot(beta.one.sims
     , -(pL.b1+max(-pL.b1))
     , "l"
     , main = TeX("Profile log-likelihood: $\\beta_1$"))
abline(h = -qchisq(0.95, df = 1)/2, lty = "dashed")
```

From these figures it can be concluded that the quadratic approximation of the CI through use of Fischers information matrix, is a sufficiently good approximation.

The Wald and likelihood-based confidence intervals for the two $\beta_i, i\in (0,1)$ parameters can be seen printed below.

```{r}
sd_0 <- as.numeric(sqrt(diag(solve(hessian(beta_0, func = nll.p_0)))))
sd_1 <- as.numeric(sqrt(diag(solve(hessian(beta_1, func = nll.p_1, beta_0 = beta_0)))))

#Wald 95% CIs and profile-likelihoods with approx 95% CI
W.CI.0 <- round(beta_0 + c(-1,1)*qnorm(0.975)*sd_0,4)
W.CI.1 <- round(beta_1 + c(-1,1)*qnorm(0.975)*sd_1,4) #KOMMENTAR: CI.theta fra den tidligere kommentar er lig dette interval. Dette er vel netop, fordi theta i den sammenhæng beskriver forskellen på, om der er treatment eller ej, hvilket er det præcis samme som beta_1.
#W.CI.1;CI.theta
#Måske, forstår stadig ikke helt theta.hat fra tidligere, så det er svært at sige. Men du har nok ret.

#Direkte numerisk approksimation:
CI.0 <- round(c(min(beta.zero.sims[-(pL.b0+max(-pL.b0)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.zero.sims[-(pL.b0+max(-pL.b0)) > -qchisq(0.95, df = 1)/2])), 4)
CI.1 <- round(c(min(beta.one.sims[-(pL.b1+max(-pL.b1)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.one.sims[-(pL.b1+max(-pL.b1)) > -qchisq(0.95, df = 1)/2])), 4)

cat(paste("Wald Confidence intervals:"
      ,paste("\nbeta_0 = ", round(beta_0, 4), " [95% CI: ", W.CI.0[1],", ", W.CI.0[2],"]")
      ,paste("\nbeta_1 = ", round(beta_1, 4), " [95% CI: ", W.CI.1[1],", ", W.CI.1[2],"]")
      ,"\n\nLikelihood-based Confidence intervals:"
      ,paste0("\nbeta_0 = ", round(beta_0, 4), " [95% CI: ", CI.0[1],", ", CI.0[2],"]")
      ,paste0("\nbeta_1 = ", round(beta_1, 4), " [95% CI: ", CI.1[1],", ", CI.1[2],"]")))
```

Comparing these to eachother, we see that the Wald CI is a very good approximation of the actual CIs. However, when comparing the estimates to our glm model we see that the $95\%$ CI for AZT is wider for the model estimate...

```{r}
confint(fit.glm)
```


Below can be seen the profile log-likelihoods for the two parameters, alongside their CIs.

```{r}
par(mfrow = c(1,2))
plot(beta.zero.sims
     , -(pL.b0+max(-pL.b0))
     , "l"
     ,main = TeX("Profile log-likelihood: $\\beta_0$")
     ,xlab = expression(beta[0])
     ,ylab = "lp - max(lp)")
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = c(W.CI.0), col = 6)
text(x = W.CI.0[1]+0.2, y = -3, "Wald CI", col = 6)
text(x = CI.0[1]+0.1, y = -2.5, "CI", col = 2)
abline(v = c(CI.0), lty = "dashed", col = 2)
plot(beta.one.sims
     , -(pL.b1+max(-pL.b1))
     , "l"
     ,main = TeX("Profile log-likelihood: $\\beta_1$")
     ,xlab = expression(beta[1])
     ,ylab = "lp - max(lp)")
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = c(W.CI.1), col = 6)
text(x = W.CI.1[1]+0.2, y = -3, "Wald CI", col = 6)
text(x = CI.1[1]+0.1, y = -2.5, "CI", col = 2)
abline(v = c(CI.1),lty = "dashed", col = 2)
```


### Analysis of the Survival Time Data

#### Read the data actg320.txt into R. If you are using RStudio you can use the "Import Dataset" button.

```{r}
#tx: Treatment indicator. 1 = New treatment, 0 = Control treatment
#event: Indicator for AIDS or death. 1 = AIDS diagnosis or death, 0 = Otherwise
#time: Time to AIDS diagnosis or death. Days
#så tiden for event = 0 må angive at personen har været med i studiet time[X] dage uden at være enten død eller fået AIDS.
actg320 <- read.table("actg320.txt", header=TRUE, sep="", 
                      as.is=TRUE)

#select time, event and tx as they are the only relevant variables in this project
actg <- actg320 %>%
  dplyr::select(time, event, tx)
```

#### How many patients got AIDS or died in the two treatment groups? What is the proportion of patients that got AIDS or died in the two group? Other relevant number that could be calculated?

```{r}
actg %>%
  group_by(tx) %>%
  summarise("Got AIDS or DIED" = sum(event),
            "Proportion" = sum(event)/n(),
            "Participants Given the Treatment" = n())
```

#### Fit an exponential distribution, using numerical methods, to the time of event (time) in the data set, remember to take into account that some of the data is censored (i.e. we only know that the time to the event is longer that the reported time). 1: Using all data (i.e. ignore the treatment effect) 2: Separately for the two treatments

Fitting an exponential model, where we account for censoring:
```{r}
#page 310 in the course textbook
```

Theta for each group individually:
```{r}
#With treatment: (p. 311)
actg_tx <- filter(actg, tx == 1)
actg_no_tx = filter(actg, tx == 0)
#this is larger than 2 and thus statistically significant (skal der sammenlignes med t-dist eller chi_squared?)
```
What we did the first time around was actaully using the analytical derivations and then using these to do the tests. I have commented the actual numerical analysis but have left the first results uncommented in order not to mess with the code. See exp.nll just below for "analytical optimization" and exp.ll.NUM for numerical optimization.

```{r}
exp.nll <- function(theta, time = actg$time, event = actg$event){ #this isn't numerical? KOMMENTAR
  nll <- -(sum(event)*log(1/theta) - sum(time/theta))
  return(nll)
}
exp.nll.NUM <- function(theta, time = actg$time, event = actg$event){ #this is 8D
  ll <- sum( dexp(time[event==1], 1/theta, log = T) ) + sum( pexp(time[event==0], 1/theta, lower.tail = F, log.p = T) )
  return(-ll) 
}

est <- nlminb(start = 1, objective = exp.nll)
est.NUM <- nlminb(start = 1, objective = exp.nll.NUM)
est.tx <- nlminb(start = 1, objective = exp.nll, time = actg_tx$time, event = actg_tx$event)
est.tx.NUM <- nlminb(start = 1, objective = exp.nll.NUM, time = actg_tx$time, event = actg_tx$event)
est.no.tx <- nlminb(start = 1, objective = exp.nll, time =actg_no_tx$time, event = actg_no_tx$event)
est.no.tx.NUM <- nlminb(start = 1, objective = exp.nll.NUM, time = actg_no_tx$time, event = actg_no_tx$event)

sd.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.NUM$par )))))
sd.tx.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.tx.NUM$par, time = actg_tx$time, event = actg_tx$event) ))))
sd.no.tx.NUM <- as.numeric(sqrt(diag(solve( hessian(exp.nll.NUM, est.no.tx.NUM$par, time = actg_no_tx$time, event = actg_no_tx$event) ))))

(est.tx.NUM$par - est.no.tx.NUM$par)/sqrt(sd.tx.NUM^2 + sd.no.tx.NUM^2) #311, compared to standard norm (z). the effect is significant :)

theta.hat.both <- est.NUM$par#to avoid having to alter all names
theta.hat.tx <- est.tx.NUM$par
theta.hat.no.tx <- est.no.tx.NUM$par

#weibull
wei.nll <- function(omega, time = actg$time, event = actg$event){
  sigma <- omega[2]
  theta <- omega[1]
  
  p <- sigma^(-1)*time^(1/sigma-1)*theta^(-1/sigma) * exp(-time/theta)^(1/sigma)
  P <- exp(-(time/theta)^(1/sigma))
  nll <- -sum(log(p^event*P^(1-event)))
}

est.wei <- nlminb(start = c(2,2), objective = wei.nll, time = actg_tx$time, event = actg_tx$event)

#cumulative hazard function:
kaplan.meier <- survfit(Surv(time, event) ~ 1, data = actg)
kaplan.meier$time
kaplan.meier$cumhaz
est.cumhaz <- pexp(kaplan.meier$time, rate = 1/est$par)
compare.table <- tibble("time" = kaplan.meier$time, "KM cumhaz" = kaplan.meier$cumhaz, "exp cumhaz" = est.cumhaz)

ggplot(compare.table,aes(x = `exp cumhaz`, y = `KM cumhaz`))+
  geom_step()+
  geom_abline()

kaplan.meier.treatment <- survfit(Surv(time, event) ~ tx, data = actg)
est.cumhaz.tx <- pexp(kaplan.meier.treatment$time[-(1:228)], rate = 1/theta.hat.tx)
compare.table.treatment <- tibble("time" = kaplan.meier.treatment$time[-(1:228)], "KM cumhaz tx" = kaplan.meier.treatment$cumhaz[-(1:228)], "exp cumhaz tx" = est.cumhaz.tx)

ggplot(compare.table.treatment,aes(x = `exp cumhaz tx`, y = `KM cumhaz tx`))+
  geom_step()+
  geom_abline()

#weibull test for tx = 1
kaplan.meier.treatment <- survfit(Surv(time, event) ~ tx, data = actg)
time <- 1:max(kaplan.meier.treatment$time[-(1:228)])
sigma <- est.wei$par[2]
theta <- est.wei$par[1]
p <- sigma^(-1)*time^(1/sigma-1)*theta^(-1/sigma) * exp(-time/theta)^(1/sigma)
P <- exp(-(time/theta)^(1/sigma))
wei.cumhaz.tx <- cumsum(p/P)[kaplan.meier.treatment$time[-(1:228)]]
compare.table.treatment <- tibble("time" = kaplan.meier.treatment$time[-(1:228)], "KM cumhaz tx" = kaplan.meier.treatment$cumhaz[-(1:228)], "weibull cumhaz tx" = wei.cumhaz.tx)

ggplot(compare.table.treatment,aes(x = `weibull cumhaz tx`, y = `KM cumhaz tx`))+
  geom_step()+
  geom_abline()+
  theme_bw()+
  ggtitle("Weibull Survival Regression Model [Treatment Group]")+
  labs(y = "Kaplan Meier Cumulative Hazard", x = "Weibull Regression Model Cumulative Hazard")
```

#### Compared the likelihood for the above models and conclude

```{r}
#numerical analysis above, analytical way below
```

#### Formulate a model where one parameter indicate the treatment effect, find the MLE and compare with the result above.

(e.g. E[T] = $e^\beta_0$ if control group and E[T] = $e^{\beta_0 + \beta_1}$ if treatment group)

```{r, fig.height=7}
kaplan.meier <- survfit(Surv(time, event) ~ tx, data = actg)
ggsurvplot_add_all(kaplan.meier
                   , data = actg
                   , conf.int = T
                   , risk.table = "abs_pct"
                   , ylim = c(0.8,1)
                   , pval = T
                   , ncensor.plot = T
                   ,ggtheme = theme_bw()
                   ,legend.labs = c("All", "No Treatment", "Treatment"))

fit <- survreg(Surv(time, event) ~ tx, data = actg,
               dist = "exponential")
summary(fit)
confint(fit)

#Overvej residual plot

#Ifølge ovenstående:
#beta0 = 7.62 95% CI [7.38; 7.87]
#beta1 = 0.699 85% CI [0.28; 1.12]
# => Significant difference.

#ifølge oven- og nedenstående er der statistisk signifikant forskel.
```

Are the effect of the treatment statistically significant?
```{r}
surv_diff <- survdiff(Surv(time, event) ~ tx, data = actg)
surv_diff
```
Yes it is significant.


Same calculations but now performed by hand:

```{r, fig.height=7}
#I hånden (jvf. slides fra uge 7):
#model: T = exp(B0 + B1*tx)*epsilon, epsilon ~ exp(1)
#Der kan opstilles to forskellige modeller afhængigt af tx = 0 eller tx = 1.
#tx = 0: E[T] = exp(b0)*epsilon
#tx = 1: E[T] = exp(b0 + b1)*epsilon

#Likelihood
nll.exp <- function(beta, time = actg$time, event = actg$event, treatment = actg$tx){
  beta0 <- beta[1]
  #dont want to make two functions so let beta1 = 0 if no treatment is not considered/used:
  if (max(treatment) == 0){
    beta1 <- 0
  } else {
    beta1 <- beta[2]
  }
  
  h <- exp(- beta0 - beta1 * treatment)
  H <- time/exp(beta0 + beta1*treatment)
  nll <- -sum(event*log(h) - H)
  return(nll)
}
l.exp <- function(beta_tr, beta_nuis, time = actg$time, event = actg$event, treatment = actg$tx){
  h <- exp(- beta_nuis - beta_tr * treatment)
  H <- time/exp(beta_nuis + beta_tr*treatment)
  ll <- sum(event*log(h) - H)
  return(ll)
}

beta_hat <- nlminb(start = c(1,1)
                   , objective = nll.exp
                   , time = actg$time
                   , event = actg$event
                   , treatment = actg$tx)
beta_hat$par


#Comparing likelihoods with the result from bullet-point 4
beta_hat$objective #Ved ikke lige om der skal sammenlignes med de to modeller eller den ene? ahh
#måske skal man undersøge om begge værdier er statistisk signifikante således at vi kan argumentere for
#at der er tale om at behandlingen virker og sammenligne dette resultat med bullet-point 4.

#Calculate LRT
#optimise model without beta1 (no treatment):
beta_no_treatment_effect <- nlminb(start = 1
                                   , objective = nll.exp
                                   , time = actg$time
                                   , event = actg$event
                                   , treatment = rep(0, length(actg$tx)))

beta_no_treatment_effect$par

#LRT:
chi_squared <- - 2 * (beta_hat$objective - beta_no_treatment_effect$objective)
(p_value <- 1 - pchisq(chi_squared, df = 1))
#Here, we see that the treatment effect is statistically significant.

#### Bullet point 6 - Wald CI for the treatment parameters beta0 and beta1 ####
#Calculate profile likelihoods to ensure that the quadratic approximation by using Fischers Information matrix
#is acceptable.

beta.zero.sims <- seq(7.3,7.9,0.01)
beta.one.sims <- seq(0.2,1.2,0.01)
pL.beta0 <- apply(X = data.frame(beta.zero.sims,beta_hat$par[2]), MARGIN = 1 , FUN = nll.exp, time = actg$time, event = actg$event, treatment = actg$tx)
pL.beta1 <- apply(X = data.frame(beta_hat$par[1],beta.one.sims), MARGIN = 1 , FUN = nll.exp, time = actg$time, event = actg$event, treatment = actg$tx)

par(mfrow=c(1,2))
plot(beta.zero.sims
     , -(pL.beta0+max(-pL.beta0))
     , "l"
     ,main = "Profile likelihood for Beta 0"
     ,xlab = expression(beta[0])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
plot(beta.one.sims
     , -(pL.beta1+max(-pL.beta1))
     , "l"
     ,main = "Profile likelihood for beta 1"
     ,xlab = expression(beta[1])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)

```


#### Find the Wald confidence interval for the treatment parameter in the model above.

```{r}
#CI:
H_tr <- hessian(l.exp, beta_hat$par[2], beta_nuis = beta_hat$par[1])
V_tr <- as.numeric(-1/H_tr)
( Wald.CI_tr <- round(beta_hat$par[2] + c(-1,1)*qnorm(0.975) * sqrt(V_tr), digits=2) )
H_nuis <- hessian(l.exp, beta_hat$par[1], beta_tr = beta_hat$par[2])
V_nuis <- as.numeric(-1/H_nuis)
( Wald.CI_nuis <- round(beta_hat$par[1] + c(-1,1)*qnorm(0.975) * sqrt(V_nuis), digits=2) )
Wald.CI <- matrix(c(Wald.CI_nuis, Wald.CI_tr), 2,2, byrow = T)#KOMMENTAR: Jeg har regnet wald på en anden måde og får nu det samme som for likelihood based CI. Det lader til, at '-1/H' og 'diag(solve())' er forskellen på de to. Den første giver det samme som likelihood based, den anden giver det samme som confint. 20 linjer nede er de alle sammen sammenlignet.
#SVAR: Den gamle metode bør være korrekt. Forskellen kommer fra om man regner H^-1 som en matrix eller et tal.
#er rimelig sikker på at den bør behandles som matrix, da det anvender mest data.
sd <- c(sqrt(V_nuis), sqrt(V_tr))
sd.former <- as.numeric(sqrt(diag(solve(hessian(beta_hat$par, func = nll.exp)))))
sd;sd.former

Wald.CI_former <- beta_hat$par + matrix(c(-1,1), 2,2, byrow = T) * matrix(qnorm(0.975)*sd.former, 2,2, byrow = F)

#Direkte numerisk approksimation:
CI.0 <- c(min(beta.zero.sims[-(pL.beta0+max(-pL.beta0)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.zero.sims[-(pL.beta0+max(-pL.beta0)) > -qchisq(0.95, df = 1)/2]))
CI.1 <- c(min(beta.one.sims[-(pL.beta1+max(-pL.beta1)) > -qchisq(0.95, df = 1)/2])
           ,max(beta.one.sims[-(pL.beta1+max(-pL.beta1)) > -qchisq(0.95, df = 1)/2]))

cat(paste("Wald Confidence intervals:"
      ,paste("\nbeta_0 = ", round(beta_hat$par[1], 4), " [95% CI: ", Wald.CI[1,1],", ", Wald.CI[1,2],"]")
      ,paste("\nbeta_1 = ", round(beta_hat$par[2], 4), " [95% CI: ", Wald.CI[2,1],", ", Wald.CI[2,2],"]")
      ,"\n\nLikelihood-based Confidence intervals:"
      ,paste0("\nbeta_0 = ", round(beta_hat$par[1], 4), " [95% CI: ", CI.0[1],", ", CI.0[2],"]")
      ,paste0("\nbeta_1 = ", round(beta_hat$par[2], 4), " [95% CI: ", CI.1[1],", ", CI.1[2],"]")))
```

Comparing with the results from the survfit function:
```{r}
confint(fit);round(Wald.CI_former, digits=2)#KOMMENTAR: se her sammenligningen omtalt i den tidligere kommentar
```

Same as the wald CI. The likelihood based CI is a bit more narrow.

#### Derive the theoretical results for the models above, including the standard error estimates, use this to formulate and implement the profile likelihood function for the treatment parameter.
Theoretical results for the models of the exponential distributions. BULLET POINT 3-4.
Probabilities can be considered the probability of some data given a distribution, however, with likelihoods we want to estimate the distribution given some data. This is done by comparing the probability of observing the data given different $\theta s$. As a function of the unknown parameter the becomes:
$$
L(\theta)=P_{\theta}(X=x)
$$
<!-- p.21 was a great inspiration #KOMMENTAR, stort P eller lille p? jeg bruger ellers lille p for pdf og stort P for cdf -->

Where naturally x is the data, and $P_{\theta}$ is the pdf with parameter $\theta$. 
Usually when we work with likelihoods we simply consider all of the observations to be independent such that we can calculate the likelihood as a product of the probabilites of each observation in data = x given the same distribution $p_{\theta}$. So if X is a vector of *n* observations and $x_i$ respresents one observation the likelihood is:
$$
L(\theta)=\Pi_{i=1}^n p_{\theta}(x_i)
$$
<!-- maybe p. 27 xD -->
This brings us to how we are to interpret the uncensored and censored values. The probability of any time value that is uncensored can simply be calculated as the probability of said time given event rate $\lambda = 1/\theta$ assuming an exponential distribution.
$$
L(\theta)_{uncensored}=p_{\theta,exp}(x_i)=\frac{1}{\theta} e^{-\frac{1}{\theta}x_i}
$$
For the uncensored values we will simply define the likelihood as the probability of observing a time value greater than the one registered for said person, thus ising the cumulative distribution function:
$$
L(\theta)_{censored}=P_{\theta,exp}(x_i > t)=1 - P_{\theta,exp}(x_i\leq t)=1 - (1-e^{-\frac{1}{\theta}x_i}) =e^{-\frac{1}{\theta}x_i}
$$
We denote censoring by $\delta_i$ for observation $x_i$ where $\delta_i=0$ indicates censoring and $\delta_i=1$ indicates uncensored data. We can rewrite the two expressions above by combining their likelihoods respectively so that the former is the combined likelihood of all uncensored data and the latter is the combined likelihood of all censored data. The combined likelihood of these is then the likelihood of our unknown parameter $\theta$ given the data X:
$$
\begin{split}
L(\theta)&=\Pi_{i=1}^n p_{\theta,exp}(x_i)^{\delta_i} \cdot P_{\theta,exp}(x_i > t)^{1-\delta_i}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta} e^{-\frac{1}{\theta}x_i}\right)^{\delta_i} \cdot \left(e^{-\frac{1}{\theta}x_i}\right)^{1-\delta_i}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i\cdot\delta_i} \cdot e^{-\frac{1}{\theta}x_i\cdot(1-\delta_i)}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i\cdot\delta_i + (-\frac{1}{\theta}x_i\cdot(1-\delta_i))}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i\cdot\delta_i -\frac{1}{\theta}x_i+\frac{1}{\theta}x_i\cdot\delta_i}\\
&=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i}\\
\end{split}
$$
As the base numbers in the two terms above are constant we can simply sum the exponents for each i:
$$
L(\theta)
=\Pi_{i=1}^n \left(\frac{1}{\theta}\right)^{\delta_i} e^{-\frac{1}{\theta}x_i}=\left(\frac{1}{\theta}\right)^{\sum\delta_i} e^{-\frac{1}{\theta}\sum x_i}
$$
<!-- p. 310 to compare result -->
To determine the MLE and the fisher information we have to determine the score function which is known as the derivative of the log-likelihood. The log-likelihood is:
$$
\begin{split}
l(\theta)&=log(L(\theta))=log\left( \left(\frac{1}{\theta}\right)^{\sum\delta_i} e^{-\frac{1}{\theta}\sum x_i}\right)\\
&=log\left( \left( \frac{1}{\theta}\right)^{\sum\delta_i} \right) + log\left(e^{-\frac{1}{\theta}\sum x_i} \right)\\
&=\sum\delta_i\cdot log \left( \frac{1}{\theta}\right) - \frac{1}{\theta}\sum x_i
\end{split}
$$
In order to easily find the score function, do note that $log(\frac{1}{\theta})=log(\theta^{-1})=-log(\theta)$:

$$
\begin{split}
S(\theta)&
=\frac{\partial}{\partial \theta}log(L(\theta))
=\frac{\partial}{\partial \theta} \left( \sum\delta_i\cdot log \left( \frac{1}{\theta}\right) - \frac{1}{\theta}\sum x_i \right)\\
&=-\sum\delta_i\cdot\frac{1}{\theta}-\left(-\frac{1}{\theta^2}\sum x_i\right)\\
&=-\frac{\sum\delta_i}{\theta}+\frac{\sum x_i}{\theta^2}
\end{split}
$$

Setting this expression for the score function equal to zero we can find the MLE:
$$
\begin{split}
S(\theta)=0 &\Leftrightarrow -\frac{\sum\delta_i}{\theta}+\frac{\sum x_i}{\theta^2} = 0\\
&\Leftrightarrow\frac{\sum x_i}{\theta^2} = \frac{\sum\delta_i}{\theta}\\
&\Leftrightarrow\sum x_i=\theta\sum\delta_i\\
&\Leftrightarrow\hat{\theta}=\frac{\sum x_i}{\sum\delta_i}
\end{split}
$$
Thus we have determined the maximum likelihood estimate using an exponential model. This result can be applied to all three cases of the task: ignoring treament effect, for treatmeant effect, and for control group. NB! as the parameter was defined as the inverse of the event rate of the exponential function we have:
$\hat{\lambda} = \frac{1}{\hat{\theta}}=\frac{\sum\delta_i}{\sum x_i}$.

```{r}
#reinit the theta hats:
theta.hat.both<-0;theta.hat.tx<-0;theta.hat.no.tx<-0

(theta.hat.both <- sum(actg$time)/sum(actg$event) ) #this ain't numerical, xD
est.NUM$par

( theta.hat.tx <- sum(actg_tx$time)/sum(actg_tx$event) )
est.tx.NUM$par

#without treatment:
(theta.hat.no.tx <- sum(actg_no_tx$time)/sum(actg_no_tx$event) )
est.no.tx.NUM$par
```
All of the analytical results correspond to the numerical results obtained earlier.

Deriving an expression for the Fisher Information to obtain estimates for the standard deviation:
Fisher Information is defined as the negative derivate of the score function:
$$
\begin{split}
I(\theta)&=-\frac{\partial}{\partial\theta}S(\theta)=-\frac{\partial}{\partial\theta}\left(-\frac{\sum\delta_i}{\theta}+\frac{\sum x_i}{\theta^2}\right)\\
&=-\left( \frac{\sum\delta_i}{\theta^2}-\frac{2\sum x_i}{\theta^3}\right)\\
&=\frac{2\sum x_i}{\theta^3}-\frac{\sum\delta_i}{\theta^2}
\end{split}
$$
Now the observed Fisher Information is then taken at the MLE $\hat{\theta}=\frac{\sum x_i}{\sum\delta_i}$:
$$
\begin{split}
I(\hat{\theta})&=\frac{2\sum x_i}{\left(\frac{\sum x_i}{\sum\delta_i}\right)^3}-\frac{\sum\delta_i}{\left(\frac{\sum x_i}{\sum\delta_i}\right)^2}\\
&=\frac{2\sum x_i\sum\delta_i^3}{\sum x_i^3}-\frac{\sum\delta_i\sum\delta_i^2}{\sum x_i^2}\\
&=\frac{2\sum\delta_i^3}{\sum x_i^2}-\frac{\sum\delta_i^3}{\sum x_i^2}\\
&=\frac{\sum\delta_i^3}{\sum x_i^2}\\
&=\hat{\theta}^{-2}\sum\delta_i\\
&=\frac{\sum\delta_i}{\hat{\theta}^2}
\end{split}
$$
The variance of $\theta$ is given by $var(\theta)=I^{-1}(\theta)$ and as such since our parameter consists of only one value no matrix has to be solved and we can simply say:
$$
se(\hat{\theta})=\sqrt{I^{-1}(\hat{\theta})}=\sqrt{\frac{\hat{\theta}^2}{\sum\delta_i}}=\frac{\hat{\theta}}{\sqrt{\sum\delta_i}}
$$
The standard errors are determined analytically and compared to the numerically computed values:
```{r}

( sigma.hat.tx <- theta.hat.tx/sum(actg$event[actg$tx==1])^(1/2) )
sd.tx.NUM

(sigma.hat.no.tx <- theta.hat.no.tx/sum(actg$event[actg$tx==0])^(1/2) )
sd.no.tx.NUM
```

<!--using test on p. 311 -->
Testing if there is significance using the Wald statistic. Exactly the same result as when the numerical values were used.
```{r}
#Wald statistic for comparison
(theta.hat.tx - theta.hat.no.tx)/sqrt(sigma.hat.tx^2 + sigma.hat.no.tx^2)
(est.tx.NUM$par - est.no.tx.NUM$par)/sqrt(sd.tx.NUM^2 + sd.no.tx.NUM^2)
```
Profile likelihood for $\theta$:
```{r}
exp.L.fun <- function(theta, time, event){
  L <- (1/theta)^sum(event) * exp(-sum(time)/theta)
  return(L)
}
thetas.tx <- seq(theta.hat.tx-2*sd.tx.NUM, theta.hat.tx+2.5*sd.tx.NUM, by = 0.02)
Ls <- sapply(thetas.tx, FUN = exp.L.fun, time = actg_tx$time, event = actg_tx$event)
plot(thetas.tx, Ls/max(Ls), main = "Profile likelihood for theta for exp dist",
     xlab=expression(theta), ylab="norm L", lwd=1.0)
abline(a=0.15, b=0, col="red")

( theta.hat.tx+c(-1,1) * qnorm(0.975) * sd.tx.NUM )
```





```{r}
( sd_tr <- sqrt(V_tr) )
par(mfrow = c(1,2))
plot(beta.zero.sims
     , -(pL.beta0+max(-pL.beta0))
     , "l"
     ,main = "Profile likelihood for Beta 0"
     ,xlab = expression(beta[0])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = Wald.CI[1,], col = 6)
text(x = Wald.CI[1,1]+.15, y = -2.4, "Wald CI", col = 6)
text(x = CI.0[1]+.05, y = -2.5, "CI", col = 2)
abline(v = c(CI.0), lty = "dashed", col = 2)
plot(beta.one.sims
     , -(pL.beta1+max(-pL.beta1))
     , "l"
     ,main = "Profile likelihood for beta 1"
     ,xlab = expression(beta[1])
     ,ylab = "lp - max(lp)"
     ,ylim = c(-3.2,0))
abline(h = -qchisq(0.95, df = 1)/2, col = 2)
abline(v = Wald.CI[2,], col = 6)
text(x = Wald.CI[2,1]+0.25, y = -1.6, "Wald CI", col = 6)
text(x = CI.1[1]+0.09, y = -1.7, "CI", col = 2)
abline(v = c(CI.1),lty = "dashed", col = 2)
```

**(Have not included our analysis based on the weibull distribution)**